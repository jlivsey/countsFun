#' #' @keywords internal
#' @importFrom stats ARMAacf arima.sim coef dbinom dnbinom dpois filter glm arima residuals
#' @importFrom stats pbinom plogis pnbinom pnorm ppois qbinom qnbinom qnorm nobs
#' @importFrom stats qpois rbinom rmultinom runif sd terms var
#' @importFrom MASS glm.nb
#' @importFrom VGAM vglm pzipois qzipois dzipois rzipois genpoisson2 loglink
#' @importFrom VGAM dgenpois2 qgenpois2 pgenpois2 rgenpois2 zipoisson
#' @importFrom iZID poisson.zihmle
#' @importFrom mixtools poisregmixEM
#' @importFrom extraDistr dmixpois pmixpois rmixpois
#' @importFrom optimx optimx
#' @importFrom optextras gHgen
#' @importFrom parallel detectCores makeCluster stopCluster
#' @importFrom doParallel registerDoParallel
#' @importFrom foreach foreach %dopar%
"_PACKAGE"


#' Create and Validate Model Specification for Particle Filtering
#'
#' Parses and validates the full model specification and specified options.
#' This function gathers model components (data, distribution,
#' dependence structure, optimization parameters, etc.) and returns a structured list used as
#' input to other core functions in the package.
#'
#' @param DependentVar Numeric vector or data frame. The response (count) time series.
#' @param Regressor Optional matrix or data frame of covariates.
#' @param Intercept Logical. Whether to include an intercept in the model.
#' @param EstMethod Character. Estimation method. Currently only \code{"PFR"} is supported.
#' @param ARMAModel Integer vector of length 2. Specifies the order of the AR and MA components.
#' @param CountDist Character. The name of the count distribution (e.g., "Poisson", "Negative Binomial", "ZIP").
#' @param ParticleNumber Integer. Number of particles to be used in the filter.
#' @param epsilon Numeric. Resampling occurs when the effective sample size (ESS) drops below \code{epsilon * N}.
#' @param initialParam Numeric vector. Initial parameter values for optimization.
#' @param TrueParam Numeric vector. True parameter values (used in simulation/synthesis, ignored otherwise).
#' @param Task Character. One of \code{"Evaluation"}, \code{"Optimization"}, \code{"Simulation"}, or \code{"Synthesis"}.
#' @param SampleSize Integer. Required when \code{Task == "Synthesis"}.
#' @param OptMethod Character. Optimization method, e.g., \code{"L-BFGS-B"}.
#' @param OutputType Character. Type of output, typically \code{"list"}.
#' @param ParamScheme Integer. Indicates the parameter configuration scheme.
#' @param maxdiff Numeric. Convergence criterion for optimization.
#' @param ntrials Integer. Number of trials (used in Binomial model).
#' @param ... Additional arguments (currently unused).
#'
#' @return A named list containing model specifications, validated inputs, marginal distribution
#' functions (PDF, CDF, inverse CDF), initial and true parameters, parameter bounds,
#' parameter names, and utility values for likelihood evaluation.
#'
#' @details
#' This function validates the inputs (e.g., distribution support, ARMA structure, initial
#' parameter lengths), computes the number of model parameters, constructs marginal distribution
#' functions based on the count distribution, and configures parameter bounds. It is a required
#' pre-processing step before fitting or simulating count time series models.
#'
#'
#' @export
ModelScheme = function(DependentVar = NULL, Regressor=NULL, Intercept = NULL, EstMethod="PFR", ARMAModel=c(0,0), CountDist="NULL",
                       ParticleNumber = 5, epsilon = 0.5, initialParam=NULL, TrueParam=NULL, Task="Evaluation", SampleSize=NULL,
                       OptMethod="L-BFGS-B", OutputType="list", ParamScheme=1, maxdiff=10^(-8),ntrials= NULL,...){

  # Distribution list
  if( !(CountDist %in% c("Poisson", "Negative Binomial", "Generalized Poisson", "Generalized Poisson 2", "Mixed Poisson",
                         "ZIP", "Binomial")))
    stop("The specified distribution in not supported.")

  # Specify Task
  #if( !(Task %in% c("Evaluation", "Optimization", "Synthesis"))){
  if( !(Task %in% c("Evaluation", "Optimization", "Simulation", "Synthesis"))){
    Task = "Evaluation"
    message("The selected Task is not supported. The Task has been set to 'Evaluation'")
  }

  # Estimation Method for Optimization and
  if( !(EstMethod %in% c("PFR")) & Task %in% c("Optimization", "Simulation") ){
    EstMethod = "PFR"
    message("The selected EstMethod is not supported. EstMethod has been set to'PFR'")
  }

  # If user specified EstMethod but asks for evaluation Task then no estimation takes place
  if (Task %in% c("Evaluation") & EstMethod %in% c("PFR")){
    EstMethod = "None"
    message("For the Evaluation Task no estimation takes place. EstMethod has been set to None. To fit the model
            specify the Optimzation Task.")
  }


  # check that the ARMA order has dimension 2, the ARMA orders are integers
  if(length(ARMAModel)!=2 | !prod(ARMAModel %% 1 == c(0,0)) )
    stop("The specified ARMA model is not supported.")

  # make the dependent variable numeric
  if(is.data.frame(DependentVar)){
    DependentVar = DependentVar[,1]
  }

  # retrieve sample size
  if(Task=="Synthesis"){
    n = SampleSize
  }else{
    n = length(DependentVar)
  }

  # fix me: do I need a warning here is there is no samplesize in simulation
  if(!is.null(Regressor)){
    if (Intercept==TRUE && sum(as.matrix(Regressor)[,1])!=n ){
      Regressor = as.data.frame(cbind(rep(1,n),Regressor))
      names(Regressor)[1] = "Intercept"
    }
    else{
      Regressor = as.data.frame(Regressor)
    }
  }

  # number of regressors
  nreg = ifelse(is.null(Regressor), 0, DIM(Regressor)[2]-as.numeric(Intercept))

  if(is.null(Intercept) || is.null(Regressor)){
    nint = 1
  }else{
    nint = as.numeric(Intercept)
  }

  # get the indices of marginal parameters
  MargParmIndices = switch(CountDist,
                           "Poisson"               = 1:(1+nreg+nint-1),
                           "Negative Binomial"     = 1:(2+nreg+nint-1),
                           "Generalized Poisson"   = 1:(2+nreg+nint-1),
                           "Generalized Poisson 2" = 1:(2+nreg+nint-1),
                           "Binomial"              = 1:(1+nreg+nint-1),
                           "Mixed Poisson"         = 1:(3+(nreg+nint-1)*2),
                           "ZIP"                   = 1:(2+nreg+nint-1)
  )

  # retrieve the number of  parameters
  nMargParms = length(MargParmIndices)
  nparms     = nMargParms + sum(ARMAModel)
  nAR        = ARMAModel[1]
  nMA        = ARMAModel[2]

  if(Task=="Synthesis"){
    if (nparms!=length(TrueParam)){
      stop("The length of the specified true parameter does not comply with the
           model or the number of regressors.")
    }
  }

  # cannot allow for sample size to be smaller than model parameters
  if (n<=(nparms+2))
    stop(sprintf("The provided sample size is equal to %.0f while the
                 the specified model has %.0f parameters",n,nparms))

  # check if initial param length is wrong
  if(!is.null(initialParam) & length(initialParam)!=nparms)
    stop("The specified initial parameter has wrong length.")

  # parse all information in the case without Regressors or in the case with Regressors
  if(nreg<1){
    # retrieve marginal cdf
    mycdf = switch(CountDist,
                   "Poisson"               = ppois,
                   "Negative Binomial"     = function(x, theta){   pnbinom(x, theta[1], 1-theta[2])},
                   "Generalized Poisson"   = function(x, theta){    pGpois(x, theta[1], theta[2])},
                   "Generalized Poisson 2" = function(x, theta){ pgenpois2(x, theta[2], theta[1])},,
                   "Binomial"              = function(x, theta){    pbinom(x, ntrials, theta[1])},
                   "Mixed Poisson"         = function(x, theta){ pmixpois1(x, theta[1], theta[2], theta[3])},
                   "ZIP"                   = function(x, theta){   pzipois(x, theta[1], theta[2])}
    )

    # retrieve marginal pdf
    mypdf = switch(CountDist,
                   "Poisson"               = dpois,
                   "Negative Binomial"     = function(x, theta){   dnbinom(x, theta[1], 1-theta[2])},
                   "Generalized Poisson"   = function(x, theta){    dGpois(x, theta[1], theta[2])},
                   "Generalized Poisson 2" = function(x, theta){ dgenpois2(x, theta[2], theta[1])},
                   "Binomial"              = function(x, theta){    dbinom(x, ntrials, theta[1])},
                   "Mixed Poisson"         = function(x, theta){ dmixpois1(x, theta[1], theta[2], theta[3])},
                   "ZIP"                   = function(x, theta){   dzipois(x, theta[1], theta[2])}
    )

    # retrieve marginal inverse cdf
    myinvcdf = switch(CountDist,
                      "Poisson"               = qpois,
                      "Negative Binomial"     = function(x, theta){   qnbinom(x, theta[1], 1-theta[2])},
                      "Generalized Poisson"   = function(x, theta){    qGpois(x, theta[1], theta[2])},
                      "Generalized Poisson 2" = function(x, theta){ qgenpois2(x, theta[2], theta[1])},
                      "Binomial"              = function(x, theta){    qbinom(x, ntrials, theta[1])},
                      "Mixed Poisson"         = function(x, theta){  qmixpois(x, theta[1], theta[2], theta[3])},
                      "ZIP"                   = function(x, theta){   qzipois(x, theta[1], theta[2])}
    )

    # lower bound constraints
    LB = switch(CountDist,
                "Poisson"               = c(0.01,              rep(-Inf, sum(ARMAModel))),
                "Negative Binomial"     = c(0.01, 0.01,        rep(-Inf, sum(ARMAModel))),
                "Generalized Poisson"   = c(0.01, 0.01,        rep(-Inf, sum(ARMAModel))),
                "Generalized Poisson 2" = c(0.01, 0.01,        rep(-Inf, sum(ARMAModel))),
                "Binomial"              = c(0.01,              rep(-Inf, sum(ARMAModel))),
                "Mixed Poisson"         = c(0.01, 0.01, 0.01,  rep(-Inf, sum(ARMAModel))),
                "ZIP"                   = c(0.01, 0.01,        rep(-Inf, sum(ARMAModel)))
    )
    # upper bound constraints
    UB = switch(CountDist,
                "Poisson"               = c(Inf,            rep( Inf, sum(ARMAModel))),
                "Negative Binomial"     = c(Inf, 0.99,      rep( Inf, sum(ARMAModel))),
                "Generalized Poisson"   = c(Inf, Inf,       rep( Inf, sum(ARMAModel))),
                "Generalized Poisson 2" = c(Inf, Inf,       rep( Inf, sum(ARMAModel))),
                "Binomial"              = c(Inf,            rep( Inf, sum(ARMAModel))),
                "Mixed Poisson"         = c(Inf, Inf, 0.99, rep( Inf, sum(ARMAModel))),
                "ZIP"                   = c(Inf, 0.99,      rep( Inf, sum(ARMAModel)))
    )
    # names of marginal parameters
    MargParmsNames = switch(CountDist,
                            "Poisson"               = c("lambda"),
                            "Negative Binomial"     = c("r","p"),
                            "Mixed Poisson"         = c("lambda_1", "lambda_2", "p"),
                            "Generalized Poisson"   = c("a", "mu"),
                            "Generalized Poisson 2" = c("a", "mu"),
                            "Binomial"              = c("p"),
                            "ZIP"                   = c("lambda", "p")
    )
  }else{
    # retrieve marginal cdf
    mycdf = switch(CountDist,
                   "Poisson"               = function(x, ConstMargParm, DynamMargParm){     ppois(x, DynamMargParm)},
                   "Negative Binomial"     = function(x, ConstMargParm, DynamMargParm){   pnbinom(x, ConstMargParm, 1-DynamMargParm)},
                   "Generalized Poisson"   = function(x, ConstMargParm, DynamMargParm){    pGpois(x, ConstMargParm, DynamMargParm)},
                   "Generalized Poisson 2" = function(x, ConstMargParm, DynamMargParm){ pgenpois2(x, DynamMargParm, ConstMargParm)},
                   "Binomial"              = function(x, ConstMargParm, DynamMargParm){    pbinom(x, ntrials, DynamMargParm)},
                   "Mixed Poisson"         = function(x, ConstMargParm, DynamMargParm){ pmixpois1(x, DynamMargParm[1], DynamMargParm[2], ConstMargParm)},
                   "ZIP"                   = function(x, ConstMargParm, DynamMargParm){   pzipois(x, DynamMargParm, ConstMargParm)}
    )
    # retrieve marginal pdf
    mypdf = switch(CountDist,
                   "Poisson"               = function(x, ConstMargParm, DynamMargParm){     dpois(x, DynamMargParm)},
                   "Negative Binomial"     = function(x, ConstMargParm, DynamMargParm){   dnbinom(x, ConstMargParm, 1-DynamMargParm)},
                   "Generalized Poisson"   = function(x, ConstMargParm, DynamMargParm){    dGpois(x, ConstMargParm, DynamMargParm)},
                   "Generalized Poisson 2" = function(x, ConstMargParm, DynamMargParm){ dgenpois2(x, DynamMargParm, ConstMargParm)},
                   "Binomial"              = function(x, ConstMargParm, DynamMargParm){    dbinom(x, ntrials, DynamMargParm)},
                   "Mixed Poisson"         = function(x, ConstMargParm, DynamMargParm){ dmixpois1(x, DynamMargParm[1], DynamMargParm[2], ConstMargParm)},
                   "ZIP"                   = function(x, ConstMargParm, DynamMargParm){   dzipois(x, DynamMargParm, ConstMargParm)}
    )
    # retrieve marginal inverse cdf
    myinvcdf = switch(CountDist,
                   "Poisson"               = function(x, ConstMargParm, DynamMargParm){     qpois(x, DynamMargParm)},
                   "Negative Binomial"     = function(x, ConstMargParm, DynamMargParm){   qnbinom(x, ConstMargParm, 1-DynamMargParm)},
                   "Generalized Poisson"   = function(x, ConstMargParm, DynamMargParm){    qGpois(x, ConstMargParm, DynamMargParm)},
                   "Generalized Poisson 2" = function(x, ConstMargParm, DynamMargParm){ qgenpois2(x, DynamMargParm, ConstMargParm)},
                   "Binomial"              = function(x, ConstMargParm, DynamMargParm){    qbinom(x, ntrials, DynamMargParm)},
                   "Mixed Poisson"         = function(x, ConstMargParm, DynamMargParm){  qmixpois(x, DynamMargParm[,1], DynamMargParm[,2], ConstMargParm)},
                   "ZIP"                   = function(x, ConstMargParm, DynamMargParm){   qzipois(x, DynamMargParm, ConstMargParm)}
    )
    # lower bound contraints
    LB = switch(CountDist,
                "Poisson"               = rep(-Inf, sum(ARMAModel)+nreg+nint),
                "Negative Binomial"     = c(rep(-Inf, nreg+nint), 0.001, rep(-Inf, sum(ARMAModel))),
                "Generalized Poisson"   = c(rep(-Inf, nreg+nint), 0.001, rep(-Inf, sum(ARMAModel))),
                "Generalized Poisson 2" = c(rep(-Inf, nreg+nint), 0.001, rep(-Inf, sum(ARMAModel))),
                "Binomial"              = rep(-Inf, sum(ARMAModel)+nreg+nint),
                "Mixed Poisson"         = c(rep(-Inf, 2*(nreg+nint)), 0.001, rep(-Inf, sum(ARMAModel))),
                "ZIP"                   = c(rep(-Inf, nreg+nint), 0.001, rep(-Inf, sum(ARMAModel)))
    )
    # upper bound constraints
    UB = switch(CountDist,
                "Poisson"               = rep(Inf, sum(ARMAModel)+nreg+nint),
                "Negative Binomial"     = c(rep(Inf, nreg+nint), Inf, rep(Inf, sum(ARMAModel))),
                "Generalized Poisson"   = c(rep(Inf, nreg+nint), Inf, rep(Inf, sum(ARMAModel))),
                "Generalized Poisson 2" = c(rep(Inf, nreg+nint), Inf, rep(Inf, sum(ARMAModel))),
                "Binomial"              = rep(Inf, sum(ARMAModel)+nreg+nint),
                "Mixed Poisson"         = c(rep(Inf, 2*(nreg+nint)), 0.49, rep(Inf, sum(ARMAModel))),
                "ZIP"                   = c(rep(Inf, nreg+nint), Inf, rep(Inf, sum(ARMAModel))),
    )
    # retrieve names of marginal parameters
    MargParmsNames = switch(CountDist,
                            "Poisson"               = paste(rep("b_",nreg),(1-nint):nreg,sep=""),
                            "Negative Binomial"     = c(paste(rep("b_",nreg),(1-nint):nreg,sep=""), "k"),
                            "Mixed Poisson"         = c(paste(rep("b_1",nreg),(1-nint):nreg,sep=""),paste(rep("b_2",nreg),(1-nint):nreg,sep=""), "p"),
                            "Generalized Poisson"   = c(paste(rep("b_",nreg),(1-nint):nreg,sep=""), "a"),
                            "Generalized Poisson 2" = c(paste(rep("b_",nreg),(1-nint):nreg,sep=""), "a"),
                            "Binomial"              = paste(rep("b_",nreg),(1-nint):nreg,sep=""),
                            "ZIP"                   = c(paste(rep("b_",nreg),(1-nint):nreg,sep=""), "p"),
    )
  }

  # check whether the provided initial estimates make sense - this is for Evaluation, Simulation, Optimization Task
  if (!is.null(initialParam)) {
    if (max(initialParam<=LB) | max(initialParam>=UB)){
      stop("The specified initial parameter is outside the feasible region.")
    }
  }

  # check whether the provided initial estimates make sense - This is for Synthesis Tasks
  if (Task=="Synthesis") {
    if (max(TrueParam<=LB) | max(TrueParam>=UB)){
      stop("The specified parameter is outside the feasible region.")
    }
  }

  # create names of the ARMA parameters
  if(nAR>0) ARNames = paste("AR_",1:ARMAModel[1], sep="")
  if(nMA>0) MANames = paste("MA_",1:ARMAModel[2], sep="")

  # put all the names together
  if(nAR>0 && nMA<1) parmnames = c(MargParmsNames, ARNames)
  if(nAR<1 && nMA>0) parmnames = c(MargParmsNames, MANames)
  if(nAR>0 && nMA>0) parmnames = c(MargParmsNames, ARNames, MANames)
  if(nAR==0 && nMA==0) parmnames = c(MargParmsNames)

  # add the parmnames on theta fix me: does this affect performance?
  if(!is.null(initialParam)) names(initialParam) = parmnames

  # value I will set the loglik when things go bad (e.g. non invertible ARMA)
  loglik_BadValue1 = 10^8

  # value I will set the loglik when things go bad (e.g. non invertible ARMA)
  loglik_BadValue2 = 10^9

  out = list(
    mycdf            = mycdf,
    mypdf            = mypdf,
    myinvcdf         = myinvcdf,
    MargParmIndices  = MargParmIndices,
    initialParam     = initialParam,
    TrueParam        = TrueParam,
    parmnames        = parmnames,
    nMargParms       = nMargParms,
    nAR              = nAR,
    nMA              = nMA,
    n                = n,
    nreg             = nreg,
    nint             = nint,
    CountDist        = CountDist,
    ARMAModel        = ARMAModel,
    ParticleNumber   = ParticleNumber,
    epsilon          = epsilon,
    nparms           = nparms,
    UB               = UB,
    LB               = LB,
    EstMethod        = EstMethod,
    DependentVar     = DependentVar,
    Regressor        = Regressor,
    Task             = Task,
    OptMethod        = OptMethod,
    OutputType       = OutputType,
    ParamScheme      = ParamScheme,
    maxdiff          = maxdiff,
    loglik_BadValue1 = loglik_BadValue1,
    loglik_BadValue2 = loglik_BadValue2,
    ntrials          = ntrials,
    Intercept        = Intercept
    )
  return(out)

}



#' Particle Filter likelihood function with Resampling for ARMA Models
#'
#' Uses particle filtering with resampling to approximate the likelihood of
#' a specified count time series model with an underlying ARMA dependence structure.
#'
#' @param theta Numeric vector. Parameter vector for the model.
#' @param mod List. A list containing all model specifications, including:
#'   \describe{
#'     \item{data}{Time series data.}
#'     \item{ParticleNumber}{Integer. Number of particles to use.}
#'     \item{Regressor}{Optional independent variable(s).}
#'     \item{CountDist}{Character string. Specifies the count marginal distribution.}
#'     \item{epsilon}{Numeric. Resample when the effective sample size (ESS) falls below \code{epsilon * N}.}
#'   }
#'
#' @return Numeric. Approximate log-likelihood of the model.
#' @export
ParticleFilter_Res_ARMA = function(theta, mod){

  old_state <- get_rand_state()
  on.exit(set_rand_state(old_state))

  # Retrieve parameters and save them in a list called Parms
  Parms = RetrieveParameters(theta,mod)

  # check for causality
  if( CheckStability(Parms$AR,Parms$MA) ) return(10^(8))

  # Initialize the negative log likelihood computation
  nloglik = ifelse(mod$nreg==0,  - log(mod$mypdf(mod$DependentVar[1],Parms$MargParms)),
                   - log(mod$mypdf(mod$DependentVar[1], Parms$ConstMargParm, Parms$DynamMargParm[1,])))

  # retrieve AR, MA orders and their max
  m = max(mod$ARMAModel)
  p = mod$ARMAModel[1]
  q = mod$ARMAModel[2]


  # Compute ARMA covariance up to lag n-1
  a        = list()
  if(!is.null(Parms$AR)){
    a$phi = Parms$AR
  }else{
    a$phi = 0
  }
  if(!is.null(Parms$MA)){
    a$theta = Parms$MA
  }else{
    a$theta = 0
  }
  a$sigma2 = 1
  gamma    = itsmr::aacvf(a,mod$n)

  # Compute coefficients of Innovations Algorithm see 5.2.16 and 5.3.9 in in Brockwell Davis book
  IA       = InnovAlg(Parms, gamma, mod)
  Theta    = IA$thetas
  Rt       = sqrt(IA$v)

  # Get the n such that |v_n-v_{n-1}|< mod$maxdiff. check me: does this guarantee convergence of Thetas?
  nTheta   = IA$n
  Theta_n  = Theta[[nTheta]]

  # allocate matrices for weights, particles and predictions of the latent series
  w        = matrix(0, mod$n, mod$ParticleNumber)
  Z        = matrix(0, mod$n, mod$ParticleNumber)
  Zhat     = matrix(0, mod$n, mod$ParticleNumber)

  # initialize particle filter weights
  w[1,]    = rep(1,mod$ParticleNumber)

  # Compute the first integral limits Limit$a and Limit$b
  Limit    = ComputeLimits(mod, Parms, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))

  # Initialize the particles using N(0,1) variables truncated to the limits computed above
  #Z[1,]    = SampleTruncNormParticles(mod, Limit$a, Limit$b, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
  Z[1,]    = SampleTruncNormParticles(mod, Limit, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))


  for (t in 2:mod$n){

    # compute Zhat_t
    #Zhat[t,] = ComputeZhat_t(m,Theta,Z,Zhat,t, Parms,p,q, nTheta, Theta_n)
    Zhat[t,] = ComputeZhat_t(mod, IA, Z, Zhat,t, Parms)

    # Compute integral limits
    Limit = ComputeLimits(mod, Parms, t, Zhat[t,], Rt)

    # Sample truncated normal particles
    #Znew  = SampleTruncNormParticles(mod, Limit$a, Limit$b, t, Zhat[t,], Rt)
    Znew  = SampleTruncNormParticles(mod, Limit, t, Zhat[t,], Rt)

    # update weights
    #w[t,] = ComputeWeights(mod, Limit$a, Limit$b, t, w[(t-1),])
    w[t,] = ComputeWeights(mod, Limit, t, w[(t-1),])

    # check me: break if I got NA weight
    if (any(is.na(w[t,]))| sum(w[t,])==0 ){
      #print(t)
      #print(w[t,])
      message(sprintf('WARNING: At t=%.0f some of the weights are either too small or sum to 0',t))
      return(10^8)
    }
    # Resample the particles using common random numbers
    old_state1 = get_rand_state()
    Znew = ResampleParticles(mod, w, t, Znew)
    set_rand_state(old_state1)

    # save the current particle
    Z[t,]   = Znew

    # update likelihood
    nloglik = nloglik - log(mean(w[t,]))
  }

  # for log-likelihood we use a bias correction--see par2.3 in Durbin Koopman, 1997
  # nloglik = nloglik- (1/(2*N))*(var(na.omit(wgh[T1,]))/mean(na.omit(wgh[T1,])))/mean(na.omit(wgh[T1,]))

  # if (nloglik==Inf | is.na(nloglik)){
  #   nloglik = 10^8
  # }


  return(nloglik)
}



#' Optimization wrapper to fit PF likelihood with resampling
#'
#' Fits the particle filter log-likelihood using resampling. This function performs
#' multiple optimizations of the particle filter (PF) likelihood: it runs one fit
#' for each choice of particle number in \code{mod$ParticleNumber}.
#'
#' @param theta Numeric vector. Initial parameter values for optimization.
#' @param mod A list created by \code{\link{ModelScheme}} that contains all necessary model settings,
#'   data, distribution information, ARMA structure, bounds, etc.
#'
#' @return A list or data frame containing parameter estimates, standard errors,
#'   log-likelihood values, AIC, BIC, AICc, and convergence diagnostics.
#'
#' @details
#' The function wraps multiple calls to a particle filtering likelihood optimizer using
#' \code{\link[optimx]{optimx}}. It handles standard error estimation via numerical
#' Hessian inversion and checks for identifiability issues.
#'
#' @seealso \code{\link{ModelScheme}}, \code{\link{FitMultiplePF_Res_New}}, \code{\link{ParticleFilter_Res_ARMA}}
#'
#' @export
FitMultiplePF_Res = function(theta, mod){

  # retrieve parameter, sample size etc
  nparts   = length(mod$ParticleNumber)
  nparms   = length(theta)
  nfit     = 1
  n        = length(mod$DependentVar)

  # allocate memory to save parameter estimates, hessian values, and loglik values
  ParmEst  = matrix(0,nrow=nfit*nparts,ncol=nparms)
  se       = matrix(NA,nrow=nfit*nparts,ncol=nparms)
  loglik   = rep(0,nfit*nparts)
  convcode = rep(0,nfit*nparts)
  kkt1     = rep(0,nfit*nparts)
  kkt2     = rep(0,nfit*nparts)

  # Each realization will be fitted nfit*nparts many times
  for (j in 1:nfit){
    set.seed(j)
    # for each fit repeat for different number of particles
    for (k in 1:nparts){
      # FIX ME: I need to somehow update this in mod. (number of particles to be used). I t now works only for 1 choice of ParticleNumber
      ParticleNumber = mod$ParticleNumber[k]

      if(mod$Task == 'Optimization'){
        # run optimization for our model --no ARMA model allowed
        optim.output <- optimx(
          par     = theta,
          fn      = ParticleFilter_Res_ARMA,
          lower   = mod$LB,
          upper   = mod$UB,
          hessian = TRUE,
          method  = mod$OptMethod,
          mod     = mod)
        loglikelihood = optim.output["value"]
      }else{
        optim.output = as.data.frame(matrix(rep(NA,8+length(theta)), ncol=8+length(theta)))
        names(optim.output) = c(mapply(sprintf, rep("p%.f",length(theta)), (1:length(theta)), USE.NAMES = FALSE),
                                "value",  "fevals", "gevals", "niter", "convcode",  "kkt1", "kkt2", "xtime")

        optim.output[,1:length(theta)] = theta
        startTime = Sys.time()
        loglikelihood = ParticleFilter_Res_ARMA(theta,mod)
        endTime = Sys.time()
        runTime = difftime(endTime, startTime, units = 'secs')
        optim.output[,(length(theta)+1)] = loglikelihood
        optim.output[,(length(theta)+2)] = 1
        optim.output[,(length(theta)+3)] = 1
        optim.output[,(length(theta)+4)] = 0
        optim.output[,(length(theta)+8)] = as.numeric(runTime)
      }


      # save estimates, loglik value and diagonal hessian
      ParmEst[nfit*(k-1)+j,]  = as.numeric(optim.output[1:nparms])
      loglik[nfit*(k-1) +j]   = optim.output$value
      convcode[nfit*(k-1) +j] = optim.output$convcode
      kkt1[nfit*(k-1) +j]     = optim.output$kkt1
      kkt2[nfit*(k-1) +j]     = optim.output$kkt2


      # compute Hessian
      # t5 = tic()
      if(mod$Task == "Optimization"){
        H = gHgen(par          = ParmEst[nfit*(k-1)+j,],
                  fn             = ParticleFilter_Res_ARMA,
                  mod            = mod)
        # t5 = tic()-t5
        # print(t5)
        # if I get all na for one row and one col of H remove it
        # H$Hn[rowSums(is.na(H$Hn)) != ncol(H$Hn), colSums(is.na(H$Hn)) != nrow(H$Hn)]

        # t6 = tic()
        if (!any(is.na(rowSums(H$Hn)))){
          # save standard errors from Hessian
          if(H$hessOK && det(H$Hn)>10^(-8)){
            se[nfit*(k-1)+j,]   = sqrt(abs(diag(solve(H$Hn))))
          }else{
            se[nfit*(k-1)+j,] = rep(NA, nparms)
          }
        }else{
          # remove the NA rows and columns from H
          Hnew = H$Hn[rowSums(is.na(H$Hn)) != ncol(H$Hn), colSums(is.na(H$Hn)) != nrow(H$Hn)]

          # find which rows are missing and which are not
          NAIndex = which(colSums(is.na(H$Hn))==nparms)
          NonNAIndex = which(colSums(is.na(H$Hn))==1)

          #repeat the previous ifelse for the reduced H matrix
          if(det(Hnew)>10^(-8)){
            se[nfit*(k-1)+j,NonNAIndex]   = sqrt(abs(diag(solve(Hnew))))
          }else{
            se[nfit*(k-1)+j,NAIndex] = rep(NA, length(NAIndex))
          }
        }
      }
      # t6 = tic()-t6
      # print(t6)

    }
  }

  # Compute model selection criteria (assuming one fit)
  Criteria = Criteria.lgc(loglik, mod)


  if(mod$OutputType=="list"){
    #  save the results in a list
    ModelOutput = list()

    # specify output list names
    # names(ModelOutput)         = c("ParamEstimates", "StdErrors", "FitStatistics", "OptimOutput")
    ModelOutput$Model          = paste(mod$CountDist,  paste("ARMA(",mod$ARMAModel[1],",",mod$ARMAModel[2],")",sep=""), sep= "-")
    ModelOutput$ParamEstimates = ParmEst
    ModelOutput$Task           = mod$Task
    if(!is.null(mod$TrueParam)) ModelOutput$TrueParam      = mod$TrueParam
    if(!is.null(mod$initialParam)) ModelOutput$initialParam   = mod$initialParam
    if(mod$Task=="Optimization") ModelOutput$StdErrors      = se
    ModelOutput$FitStatistics  = c(loglik, Criteria)
    if(mod$Task=="Optimization") ModelOutput$OptimOutput    = c(convcode,kkt1,kkt2)
    ModelOutput$EstMethod      = mod$EstMethod
    ModelOutput$SampleSize     = mod$n
    if(loglikelihood==mod$loglik_BadValue1) ModelOutput$WarnMessage = "WARNING: The ARMA polynomial must be causal and invertible."
    if(loglikelihood==mod$loglik_BadValue2) ModelOutput$WarnMessage = "WARNING: Some of the weights are either too small or sum to 0."
    # assign names to all output elements
    if(!is.null(mod$TrueParam)) names(ModelOutput$TrueParam)      = mod$parmnames
    colnames(ModelOutput$ParamEstimates) = mod$parmnames

    if(mod$Task=="Optimization")colnames(ModelOutput$StdErrors)      = paste("se(", mod$parmnames,")", sep="")
    names(ModelOutput$FitStatistics)     = c("loglik", "AIC", "BIC", "AICc")
    if(mod$Task=="Optimization")names(ModelOutput$OptimOutput)       = c("ConvergeStatus", "kkt1", "kkt2")

  }else{
    ModelOutput  = data.frame(matrix(ncol = 4*mod$nparms+16, nrow = 1))

    # specify output names
    if(!is.null(mod$TrueParam)){
      colnames(ModelOutput) = c(
        'CountDist','ARMAModel', 'Regressor',
        paste("True_", mod$parmnames, sep=""), paste("InitialEstim_", mod$parmnames, sep=""),
        mod$parmnames, paste("se(", mod$parmnames,")", sep=""),
        'EstMethod', 'SampleSize', 'ParticleNumber', 'epsilon', 'OptMethod', 'ParamScheme',
        "loglik", "AIC", "BIC", "AICc", "ConvergeStatus", "kkt1", "kkt2")
    }
    colnames(ModelOutput) = c(
      'CountDist','ARMAModel', 'Regressor',
      paste("InitialEstim_", mod$parmnames, sep=""),
      mod$parmnames, paste("se(", mod$parmnames,")", sep=""),
      'EstMethod', 'SampleSize', 'ParticleNumber', 'epsilon', 'OptMethod',
      "loglik", "AIC", "BIC", "AICc", "ConvergeStatus", "kkt1", "kkt2")

    # Start Populating the output data frame
    ModelOutput$CountDist      = mod$CountDist
    ModelOutput$ARMAModel      = paste("ARMA(",mod$ARMAModel[1],",",mod$ARMAModel[2],")",sep="")
    ModelOutput$Regressor      = !is.null(mod$Regressor)

    offset = 4
    # true Parameters
    if(!is.null(mod$TrueParam)){
      if(mod$nMargParms>0){
        ModelOutput[, offset:(offset + mod$nMargParms -1)] = mod$TrueParam[1:mod$nMargParms]
        offset = offset + mod$nMargParms
      }

      if(mod$nAR>0){
        ModelOutput[, offset:(offset + mod$nAR -1)]        = mod$TrueParam[ (mod$nMargParms+1):(mod$nMargParms+mod$nAR) ]
        offset = offset + mod$nAR
      }

      if(mod$nMA>0){
        ModelOutput[, offset:(offset + mod$nMA -1)]        = mod$TrueParam[ (mod$nMargParms+mod$nAR+1):(mod$nMargParms+mod$nAR+mod$nMA)]
        offset = offset + mod$nMA
      }
    }

    # Initial Parameter Estimates
    if(mod$nMargParms>0){
      ModelOutput[, offset:(offset + mod$nMargParms -1 )] = mod$initialParam[1:mod$nMargParms ]
      offset = offset + mod$nMargParms
    }
    if(mod$nAR>0){
      ModelOutput[, offset:(offset + mod$nAR-1)]        = mod$initialParam[(mod$nMargParms+1):(mod$nMargParms+mod$nAR) ]
      offset = offset + mod$nAR
    }

    if(mod$nMA>0){
      ModelOutput[, offset:(offset + mod$nMA-1)]        = mod$initialParam[(mod$nMargParms+mod$nAR+1):(mod$nMargParms+mod$nAR+mod$nMA) ]
      offset = offset + mod$nMA
    }

    # Parameter Estimates
    if(mod$nMargParms>0){
      ModelOutput[, offset:(offset + mod$nMargParms-1)] = ParmEst[,1:mod$nMargParms]
      offset = offset + mod$nMargParms
    }

    if(mod$nAR>0){
      ModelOutput[, offset:(offset + mod$nAR-1)]        = ParmEst[,(mod$nMargParms+1):(mod$nMargParms+mod$nAR)]
      offset = offset + mod$nAR
    }

    if(mod$nMA>0){
      ModelOutput[, offset:(offset + mod$nMA-1)]        = ParmEst[,(mod$nMargParms+mod$nAR+1):(mod$nMargParms+mod$nAR+mod$nMA)]
      offset = offset + mod$nMA
    }

    # Parameter Std Errors
    if(mod$nMargParms>0){
      ModelOutput[, offset:(offset + mod$nMargParms-1)] = se[,1:mod$nMargParms]
      offset = offset + mod$nMargParms
    }

    if(mod$nAR>0){
      ModelOutput[, offset:(offset + mod$nAR-1)]        = se[,(mod$nMargParms+1):(mod$nMargParms+mod$nAR)]
      offset = offset + mod$nAR
    }

    if(mod$nMA>0){
      ModelOutput[, offset:(offset + mod$nMA-1)]        = se[,(mod$nMargParms+mod$nAR+1):(mod$nMargParms+mod$nAR+mod$nMA)]
    }

    ModelOutput$EstMethod      = mod$EstMethod
    ModelOutput$SampleSize     = mod$n
    ModelOutput$ParticleNumber = mod$ParticleNumber
    ModelOutput$epsilon        = mod$epsilon
    ModelOutput$OptMethod      = mod$OptMethod
    if(!is.null(mod$TrueParam)) ModelOutput$ParamScheme    = mod$ParamScheme
    ModelOutput$loglik         = loglik
    ModelOutput$AIC            = Criteria[1]
    ModelOutput$BIC            = Criteria[2]
    ModelOutput$AICc           = Criteria[3]
    ModelOutput$ConvergeStatus = convcode
    ModelOutput$kkt1           = kkt1
    ModelOutput$kkt2           = kkt2
  }

  return(ModelOutput)
}


#' Optimization wrapper to fit PF likelihood with resampling (new version)
#'
#' Fits the particle filter log-likelihood using resampling. This version expects
#' a complete model object (created by \code{\link{ModelScheme}}) and performs
#' one optimization per value of \code{mod$ParticleNumber}.
#'
#' @param theta Numeric vector. Initial parameter values.
#' @param mod A list containing model-related metadata and control settings, typically
#'   returned by \code{\link{ModelScheme}}.
#'
#' @return A list containing parameter estimates, standard errors, log-likelihood,
#'   AIC, BIC, AICc, and optimization diagnostics.
#'
#' @export
FitMultiplePF_Res_New = function(theta, mod){

  # retrieve parameter, sample size etc
  nparts   = length(mod$ParticleNumber)
  nparms   = length(theta)
  nfit     = 1
  n        = length(mod$DependentVar)

  # allocate memory to save parameter estimates, hessian values, and loglik values
  ParmEst  = matrix(0,nrow=nfit*nparts,ncol=nparms)
  se       = matrix(NA,nrow=nfit*nparts,ncol=nparms)
  loglik   = rep(0,nfit*nparts)
  convcode = rep(0,nfit*nparts)
  kkt1     = rep(0,nfit*nparts)
  kkt2     = rep(0,nfit*nparts)

  # Each realization will be fitted nfit*nparts many times
  for (j in 1:nfit){
    set.seed(j)
    # for each fit repeat for different number of particles
    for (k in 1:nparts){
      # FIX ME: I need to somehow update this in mod. (number of particles to be used). I t now works only for 1 choice of ParticleNumber
      ParticleNumber = mod$ParticleNumber[k]

      if(mod$Task == 'Optimization'){
        # run optimization for our model --no ARMA model allowed
        optim.output <- optimx(
          par     = theta,
          fn      = ParticleFilter_Res_ARMA,
          lower   = mod$LB,
          upper   = mod$UB,
          hessian = TRUE,
          method  = mod$OptMethod,
          mod     = mod)
        loglikelihood = optim.output["value"]
      }else{
        optim.output = as.data.frame(matrix(rep(NA,8+length(theta)), ncol=8+length(theta)))
        names(optim.output) = c(mapply(sprintf, rep("p%.f",length(theta)), (1:length(theta)), USE.NAMES = FALSE),
                                "value",  "fevals", "gevals", "niter", "convcode",  "kkt1", "kkt2", "xtime")

        optim.output[,1:length(theta)] = theta
        startTime = Sys.time()
        loglikelihood = ParticleFilter_Res_ARMA(theta,mod)
        endTime = Sys.time()
        runTime = difftime(endTime, startTime, units = 'secs')
        optim.output[,(length(theta)+1)] = loglikelihood
        optim.output[,(length(theta)+2)] = 1
        optim.output[,(length(theta)+3)] = 1
        optim.output[,(length(theta)+4)] = 0
        optim.output[,(length(theta)+8)] = as.numeric(runTime)
      }


      # save estimates, loglik value and diagonal hessian
      ParmEst[nfit*(k-1)+j,]  = as.numeric(optim.output[1:nparms])
      loglik[nfit*(k-1) +j]   = optim.output$value
      convcode[nfit*(k-1) +j] = optim.output$convcode
      kkt1[nfit*(k-1) +j]     = optim.output$kkt1
      kkt2[nfit*(k-1) +j]     = optim.output$kkt2


      # compute Hessian
      # t5 = tic()
      if(mod$Task == "Optimization"){
        H = gHgen(par          = ParmEst[nfit*(k-1)+j,],
                  fn             = ParticleFilter_Res_ARMA,
                  mod            = mod)
        # t5 = tic()-t5
        # print(t5)
        # if I get all na for one row and one col of H remove it
        # H$Hn[rowSums(is.na(H$Hn)) != ncol(H$Hn), colSums(is.na(H$Hn)) != nrow(H$Hn)]

        # t6 = tic()
        if (!any(is.na(rowSums(H$Hn)))){
          # save standard errors from Hessian
          if(H$hessOK && det(H$Hn)>10^(-8)){
            se[nfit*(k-1)+j,]   = sqrt(abs(diag(solve(H$Hn))))
          }else{
            se[nfit*(k-1)+j,] = rep(NA, nparms)
          }
        }else{
          # remove the NA rows and columns from H
          Hnew = H$Hn[rowSums(is.na(H$Hn)) != ncol(H$Hn), colSums(is.na(H$Hn)) != nrow(H$Hn)]

          # find which rows are missing and which are not
          NAIndex = which(colSums(is.na(H$Hn))==nparms)
          NonNAIndex = which(colSums(is.na(H$Hn))==1)

          #repeat the previous ifelse for the reduced H matrix
          if(det(Hnew)>10^(-8)){
            se[nfit*(k-1)+j,NonNAIndex]   = sqrt(abs(diag(solve(Hnew))))
          }else{
            se[nfit*(k-1)+j,NAIndex] = rep(NA, length(NAIndex))
          }
        }
      }
      # t6 = tic()-t6
      # print(t6)

    }
  }

  # Compute model selection criteria (assuming one fit)
  Criteria = Criteria.lgc(loglik, mod)

  FitResults             = list()
  FitResults$ParmEst     = ParmEst
  FitResults$se          = se
  FitResults$FitStatistics  = c(loglik, Criteria)
  FitResults$OptimOutput = c(convcode,kkt1,kkt2)

  names(FitResults$FitStatistics)     = c("loglik", "AIC", "BIC", "AICc")
  if(mod$Task=="Optimization")names(FitResults$OptimOutput)       = c("ConvergeStatus", "kkt1", "kkt2")

  return(FitResults)
}


#' Prepare Output from Model Estimation Wrapper
#'
#' Formats the output of a model fitting or evaluation task. Depending on the
#' value of \code{mod$OutputType}, the function returns either a structured
#' list (default) or a wide-format data frame containing parameter estimates,
#' standard errors, model configuration, and fit statistics.
#'
#' @param mod A list of model specifications and settings, typically created using
#'   \code{\link{ModelScheme}}. Contains all inputs used in estimation or simulation,
#'   such as the count distribution, ARMA structure, optimization method, and task type.
#' @param FitResults A list containing outputs from the model fitting process.
#'   Expected components include:
#'   \itemize{
#'     \item \code{ParmEst}: A matrix of parameter estimates.
#'     \item \code{se}: A matrix of standard errors (if applicable).
#'     \item \code{FitStatistics}: A named vector of fit measures (e.g., log-likelihood, AIC).
#'     \item \code{OptimOutput}: A list or vector of optimizer diagnostics (e.g., convergence code, KKT conditions).
#'   }
#'
#' @return If \code{mod$OutputType == "list"}, returns a named list with:
#' \itemize{
#'   \item \code{ParamEstimates}: Parameter estimates matrix.
#'   \item \code{StdErrors}: Standard error matrix (if \code{mod$Task == "Optimization"}).
#'   \item \code{FitStatistics}: Log-likelihood, AIC, BIC, AICc, etc.
#'   \item \code{OptimOutput}: Optimizer output (if applicable).
#'   \item \code{Model}: Description of the model (e.g., distribution + ARMA order).
#'   \item \code{Task}, \code{EstMethod}, \code{SampleSize}, etc.
#'   \item \code{WarnMessage}: A message indicating specific issues during fitting (if any).
#' }
#'
#' If \code{mod$OutputType != "list"}, returns a single-row data frame where each column
#' represents a parameter, fit statistic, or model metadata field.
#'
#' @details
#' This function is designed to be called within a wrapper or high-level function that
#' runs particle filtering, optimization, or simulation. It ensures consistent formatting
#' and labeling of outputs for downstream use (e.g., summaries, diagnostics, plots).
#'
#'
#' @export
PrepareOutput = function(mod, FitResults){


  if(mod$OutputType=="list"){
    #  save the results in a list
    ModelOutput = list()

    # populate information from fitting
    ModelOutput$ParamEstimates = FitResults$ParmEst
    ModelOutput$FitStatistics     = FitResults$FitStatistics

    if(mod$Task=="Optimization"){
      ModelOutput$StdErrors    = FitResults$se
      ModelOutput$OptimOutput = FitResults$OptimOutput
    }

    # populate information from parsing the initial inputs
    ModelOutput$Model          = paste(mod$CountDist,  paste("ARMA(",mod$ARMAModel[1],",",mod$ARMAModel[2],")",sep=""), sep= "-")
    ModelOutput$Task           = mod$Task
    if(!is.null(mod$TrueParam)) ModelOutput$TrueParam       = mod$TrueParam
    if(!is.null(mod$initialParam)) ModelOutput$initialParam = mod$initialParam
    ModelOutput$EstMethod      = mod$EstMethod
    ModelOutput$SampleSize     = mod$n


    if(FitResults$FitStatistics["loglik"]==mod$loglik_BadValue1) ModelOutput$WarnMessage = "WARNING: The ARMA polynomial must be causal and invertible."
    if(FitResults$FitStatistics["loglik"]==mod$loglik_BadValue2) ModelOutput$WarnMessage = "WARNING: Some of the weights are either too small or sum to 0."
    # assign names to all output elements
    if(!is.null(mod$TrueParam)) names(ModelOutput$TrueParam)      = mod$parmnames
    colnames(ModelOutput$ParamEstimates) = mod$parmnames

    if(mod$Task=="Optimization")colnames(ModelOutput$StdErrors)      = paste("se(", mod$parmnames,")", sep="")
    #names(ModelOutput$FitStatistics)     = c("loglik", "AIC", "BIC", "AICc")
    #if(mod$Task=="Optimization")names(ModelOutput$OptimOutput)       = c("ConvergeStatus", "kkt1", "kkt2")

    ModelOutput$mod            = mod
  }else{
    ModelOutput  = data.frame(matrix(ncol = 4*mod$nparms+16, nrow = 1))

    # specify output names
    if(!is.null(mod$TrueParam)){
      colnames(ModelOutput) = c(
        'CountDist','ARMAModel', 'Regressor',
        paste("True_", mod$parmnames, sep=""), paste("InitialEstim_", mod$parmnames, sep=""),
        mod$parmnames, paste("se(", mod$parmnames,")", sep=""),
        'EstMethod', 'SampleSize', 'ParticleNumber', 'epsilon', 'OptMethod', 'ParamScheme',
        "loglik", "AIC", "BIC", "AICc", "ConvergeStatus", "kkt1", "kkt2")
    }
    colnames(ModelOutput) = c(
      'CountDist','ARMAModel', 'Regressor',
      paste("InitialEstim_", mod$parmnames, sep=""),
      mod$parmnames, paste("se(", mod$parmnames,")", sep=""),
      'EstMethod', 'SampleSize', 'ParticleNumber', 'epsilon', 'OptMethod',
      "loglik", "AIC", "BIC", "AICc", "ConvergeStatus", "kkt1", "kkt2")

    # Start Populating the output data frame
    ModelOutput$CountDist      = mod$CountDist
    ModelOutput$ARMAModel      = paste("ARMA(",mod$ARMAModel[1],",",mod$ARMAModel[2],")",sep="")
    ModelOutput$Regressor      = !is.null(mod$Regressor)

    offset = 4
    # true Parameters
    if(!is.null(mod$TrueParam)){
      if(mod$nMargParms>0){
        ModelOutput[, offset:(offset + mod$nMargParms -1)] = mod$TrueParam[1:mod$nMargParms]
        offset = offset + mod$nMargParms
      }

      if(mod$nAR>0){
        ModelOutput[, offset:(offset + mod$nAR -1)]        = mod$TrueParam[ (mod$nMargParms+1):(mod$nMargParms+mod$nAR) ]
        offset = offset + mod$nAR
      }

      if(mod$nMA>0){
        ModelOutput[, offset:(offset + mod$nMA -1)]        = mod$TrueParam[ (mod$nMargParms+mod$nAR+1):(mod$nMargParms+mod$nAR+mod$nMA)]
        offset = offset + mod$nMA
      }
    }

    # Initial Parameter Estimates
    if(mod$nMargParms>0){
      ModelOutput[, offset:(offset + mod$nMargParms -1 )] = mod$initialParam[1:mod$nMargParms ]
      offset = offset + mod$nMargParms
    }
    if(mod$nAR>0){
      ModelOutput[, offset:(offset + mod$nAR-1)]        = mod$initialParam[(mod$nMargParms+1):(mod$nMargParms+mod$nAR) ]
      offset = offset + mod$nAR
    }

    if(mod$nMA>0){
      ModelOutput[, offset:(offset + mod$nMA-1)]        = mod$initialParam[(mod$nMargParms+mod$nAR+1):(mod$nMargParms+mod$nAR+mod$nMA) ]
      offset = offset + mod$nMA
    }

    # Parameter Estimates
    if(mod$nMargParms>0){
      ModelOutput[, offset:(offset + mod$nMargParms-1)] = FitResults$ParmEst[,1:mod$nMargParms]
      offset = offset + mod$nMargParms
    }

    if(mod$nAR>0){
      ModelOutput[, offset:(offset + mod$nAR-1)]        = FitResults$ParmEst[,(mod$nMargParms+1):(mod$nMargParms+mod$nAR)]
      offset = offset + mod$nAR
    }

    if(mod$nMA>0){
      ModelOutput[, offset:(offset + mod$nMA-1)]        = FitResults$ParmEst[,(mod$nMargParms+mod$nAR+1):(mod$nMargParms+mod$nAR+mod$nMA)]
      offset = offset + mod$nMA
    }

    # Parameter Std Errors
    if(mod$nMargParms>0){
      ModelOutput[, offset:(offset + mod$nMargParms-1)] = se[,1:mod$nMargParms]
      offset = offset + mod$nMargParms
    }

    if(mod$nAR>0){
      ModelOutput[, offset:(offset + mod$nAR-1)]        = se[,(mod$nMargParms+1):(mod$nMargParms+mod$nAR)]
      offset = offset + mod$nAR
    }

    if(mod$nMA>0){
      ModelOutput[, offset:(offset + mod$nMA-1)]        = se[,(mod$nMargParms+mod$nAR+1):(mod$nMargParms+mod$nAR+mod$nMA)]
    }

    ModelOutput$EstMethod      = mod$EstMethod
    ModelOutput$SampleSize     = mod$n
    ModelOutput$ParticleNumber = mod$ParticleNumber
    ModelOutput$epsilon        = mod$epsilon
    ModelOutput$OptMethod      = mod$OptMethod
    if(!is.null(mod$TrueParam)) ModelOutput$ParamScheme    = mod$ParamScheme
    ModelOutput$loglik         = FitResults$FitStatistics["loglkik"]
    ModelOutput$AIC            = FitResults$FitStatistics["AIC"]
    ModelOutput$BIC            = FitResults$FitStatistics["BIC"]
    ModelOutput$AICc           = FitResults$FitStatistics["AICc"]
    ModelOutput$ConvergeStatus = FitResults$OptimOutput["ConvergeStatus"]
    ModelOutput$kkt1           = FitResults$OptimOutput["kkt1"]
    ModelOutput$kkt2           = FitResults$OptimOutput["kkt2"]
  }

  return(ModelOutput)

}

#' Get current random seed state
#' Add some functions that I will need for common random numbers
#'
#' @keywords internal
get_rand_state <- function() {
  # Using `get0()` here to have `NULL` output in case object doesn't exist.
  # Also using `inherits = FALSE` to get value exactly from global environment
  # and not from one of its parent.
  get0(".Random.seed", envir = .GlobalEnv, inherits = FALSE)
}

#' Set random seed state
#'
#' @keywords internal
set_rand_state <- function(state) {
  # Assigning `NULL` state might lead to unwanted consequences
  if (!is.null(state)) {
    assign(".Random.seed", state, envir = .GlobalEnv, inherits = FALSE)
  }
}


#' Check Causality and Invertibility of ARMA Model
#'
#' Checks whether the specified ARMA model is stable — i.e., whether the AR polynomial
#' is causal and the MA polynomial is invertible. This is done by examining the
#' location of the roots of the characteristic polynomials.
#'
#' @param AR Numeric vector. Autoregressive (AR) coefficients of the model. Can be \code{NULL} if the model has no AR part.
#' @param MA Numeric vector. Moving average (MA) coefficients of the model. Can be \code{NULL} if the model has no MA part.
#'
#' @return Integer. Returns \code{1} if the model is **not stable** (i.e., at least one root of the AR or MA polynomial
#'          lies inside the unit circle), and \code{0} if the model is stable.
#'
#' @details
#' The function uses \code{\link[base]{polyroot}} to compute the roots of the AR and MA characteristic polynomials:
#' \itemize{
#'   \item The AR polynomial is defined as \eqn{1 - AR_1 z - AR_2 z^2 - \dots}.
#'   \item The MA polynomial is defined similarly.
#' }
#' If any root lies **within** the unit circle (i.e., has modulus < 1), the model is considered unstable.
#'
#' @examples
#' # Stable ARMA(1,1)
#' CheckStability(AR = 0.5, MA = 0.3)  # returns 0
#'
#' # Unstable ARMA(1,1)
#' CheckStability(AR = 1.2, MA = 0.3)  # returns 1
#'
#' # AR-only model
#' CheckStability(AR = c(0.9), MA = NULL)
#'
#' # MA-only model
#' CheckStability(AR = NULL, MA = c(1.5))
#'
#' @export
CheckStability = function(AR,MA){
  if (is.null(AR) && is.null(MA)) return(0)

  # return 1 if model is not stable (causal and invertible)
  if(!is.null(AR) && is.null(MA)){
    rc = ifelse(any(abs( polyroot(c(1, -AR))  ) < 1), 1,0)
  }

  if(!is.null(MA) && is.null(AR)){
    rc = ifelse(any(abs( polyroot(c(1, -MA))  ) < 1),1,0)
  }

  if(!is.null(MA) && !is.null(AR)){
    rc = ifelse(any(abs( polyroot(c(1, -AR))  ) < 1) || any(abs( polyroot(c(1, -MA))  ) < 1)   , 1, 0)
  }

  return(rc)
}


#' Compute Initial Parameter Estimates
#'
#' Generates initial parameter estimates based on the model specifications provided in \code{mod}.
#' These starting values are typically used as input to optimization routines or simulations in
#' count time series models with ARMA dependencies and various marginal count distributions.
#'
#' @param mod A list containing the full model specification, typically produced by
#'   \code{\link{ModelScheme}}. It must include fields such as the count distribution
#'   (\code{CountDist}), ARMA structure (\code{ARMAModel}), number of regressors,
#'   and any relevant data needed to construct initial estimates.
#'
#' @return A numeric vector of initial parameter estimates consistent with the model structure
#'   described in \code{mod}. The vector includes:
#'   \itemize{
#'     \item Marginal distribution parameters
#'     \item Regression coefficients (if any)
#'     \item AR and MA coefficients
#'   }
#'   The names of the vector elements (if assigned) match those in \code{mod$parmnames}.
#'
#' @details
#' The initial estimates are heuristically determined and may depend on the distribution
#' type, sample statistics (e.g., mean, variance), or defaults chosen for stability.
#' These estimates are used as starting points in numerical optimization procedures.
#'
#' The function may include logic to handle different parameterization schemes
#' (e.g., for ZIP, Negative Binomial, or Generalized Poisson models) as well as
#' constraints on parameter domains.
#'
#' @examples
#' mod <- ModelScheme(
#'   DependentVar = rpois(100, lambda = 3),
#'   CountDist = "Poisson",
#'   ARMAModel = c(1, 1),
#'   Intercept = TRUE
#' )
#' init <- InitialEstimates(mod)
#' print(init)
#'
#' @export
InitialEstimates = function(mod){
  # require(itsmr)

  # allocate memory
  est  = rep(NA, mod$nMargParms+sum(mod$ARMAModel))

  #------------First compute estimates of marginal parameters
  #-----Poisson case
  if(mod$CountDist=="Poisson"){
    if(mod$nreg==0){
      # Method of moments estimate for Poisson parameter (via the mean)
      est[1] = mean(mod$DependentVar)
    }else{
      # GLM for the mean that depends on time

      # CHECK ME: If I fit a Poisson AR(3) in the the data example of the JASA paper, but the code below doesn't
      # specify poisson family (it would pick up the default distribution that glm function has) then there will
      # be a numerical error in the likelihood. Check it!

      # to fix #36 I will introduce an ifelse below. Another way to do things would be to pass the dataframe
      # through the mod structure and use the data frame and the variable names in classic lm fashion. However,
      # with my design of passing the DependentVar and the Regrerssor as separate arguments in mod, I need the
      # ifelse statement below.

      # glmPoisson            = glm(mod$DependentVar~mod$Regressor[,2:(mod$nreg+1)], family = "poisson")
      if(mod$nint){
        glmPoisson            = glm(mod$DependentVar~ as.matrix(mod$Regressor[,2:(mod$nreg+1)]), family = "poisson")
      }else{
        glmPoisson            = glm(mod$DependentVar~0 + as.matrix(mod$Regressor[,1:mod$nreg]), family = "poisson")
      }

      est[1:mod$nMargParms] = as.numeric(glmPoisson[1]$coef)

    }
  }

  #-----Neg Binomial case
  if(mod$CountDist=="Negative Binomial"){
    if(mod$nreg==0){
      xbar = mean(mod$DependentVar)
      sSquare = var(mod$DependentVar)

      # Method of Moments for negBin
      rEst = xbar^2/(sSquare - xbar)
      pEst = 1 - xbar/sSquare
      est[1:2] = c(rEst, pEst)

    }else{
      # GLM for the mean that depends on time
      #glmNB                     = glm.nb(mod$DependentVar~mod$Regressor[,2:(mod$nreg+1)])

      if(mod$nint){
        glmNB            = glm.nb(mod$DependentVar ~ as.matrix(mod$Regressor[,2:(mod$nreg+1)]))
      }else{
        glmNB            = glm.nb(mod$DependentVar ~ 0 + as.matrix(mod$Regressor[,1:mod$nreg]))
      }

      # check me: I think that glmNegBin is a typo below and I should be using glmNB
      # est[1:(mod$nMargParms-1)] = as.numeric(glmNegBin[1]$coef)
      est[1:(mod$nMargParms-1)] = as.numeric(glmNB[1]$coef)

      # MoM for the over dispersion param in NegBin2 parametrization
      est[mod$nMargParms]       = sum(glmNB$fitted.values^2)/(sum((mod$DependentVar-glmNB$fitted.values)^2-glmNB$fitted.values))

    }
  }

  if(mod$CountDist=="Mixed Poisson"){
    if(mod$nreg==0){
      # pmle for marginal parameters
      MixPois_PMLE <- pmle.pois(mod$DependentVar,2)

      pEst  = MixPois_PMLE[[1]][1]
      l1Est = MixPois_PMLE[[2]][1]
      l2Est = MixPois_PMLE[[2]][2]

      # correct estimates if they are outside the feasible region
      # if(pEst<mod$LB[1]){pEst = 1.1*mod$LB[1]}
      # if(pEst>mod$UB[1]){pEst = 0.9*mod$UB[1]}
      #
      # if(l1Est<mod$LB[2]){l1Est = 1.1*mod$LB[2]}
      # if(l2Est<mod$LB[3]){l2Est = 1.1*mod$LB[3]}

      est[1:3] = c(l1Est, l2Est, pEst)
    }else{
      #library(mixtools)
      # MP_fit = poisregmixEM(mod$DependentVar, mod$Regressor[,2:(mod$nreg+1)])

      if(mod$nint){
        MP_fit            = poisregmixEM(mod$DependentVar, as.matrix(mod$Regressor[,2:(mod$nreg+1)]))
      }else{
        MP_fit            = poisregmixEM(mod$DependentVar, as.matrix(mod$Regressor[,1:mod$nreg]),addintercept = FALSE)
      }

      if(MP_fit$lambda[1]<0.5){
        est[1:mod$nMargParms] = as.numeric(c(MP_fit$beta, MP_fit$lambda[1]))
      }
      else{
        # check me: should I give an error here?
        # check me: does this work for more regressors? I think there will be an error with the indices below
        if(mod$nint){
          est[1:mod$nMargParms] = as.numeric(c(MP_fit$beta[3:4],MP_fit$beta[1:2], 1-MP_fit$lambda[1]))
        }else{
          est[1:mod$nMargParms] = as.numeric(c(MP_fit$beta[2],MP_fit$beta[1], 1-MP_fit$lambda[1]))
        }
        }
      #could also use the flexmix package
      #check me: for now we allow mixture of only two components
      #MP_fit <- flexmix(mod$DependentVar ~ mod$Regressor[,2:(mod$nreg+1)], k = 2, model = FLXMRglm(family = "poisson"))
      #refit1 = refit(MP_fit)

      #beta1Hat = as.numeric(refit1@coef[1:2])
      #beta2Hat = as.numeric(refit1@coef[3:4])
      #ProbHat  = MP_fit@prior[1]

      #est[1:mod$nMargParms] = c(beta1Hat, beta2Hat,ProbHat)

    }
  }

  #-----Generalized Poisson case
  if(mod$CountDist=="Generalized Poisson" || mod$CountDist=="Generalized Poisson 2"){
    if(mod$nreg==0){
      xbar    = mean(mod$DependentVar)
      sSquare = var(mod$DependentVar)


      # the GenPois density is parametrized through the mean with the pair (alpha,mu) - see (2.4) in Famoye 1994
      # solve (2.5) in Famoye 1994 wrt alpha and replace sample estimates
      alpha_1 = (sqrt(sSquare/xbar) - 1)/xbar
      alpha_2 = (-sqrt(sSquare/xbar) - 1)/xbar

      # to choose between the two solutions I ll check "overdispersion"
      if (sSquare>=xbar) alpha=alpha_1
      if (sSquare<xbar) alpha =alpha_2

      # FIX ME: the GenPois densities in VGAM do not allow for negative alpha yet
      if (alpha<0) alpha = 10^(-6)
      est[1:2] = c(alpha, xbar)

    }else{
      # run GenPois GLM using VGAM package - CHECK ME: shouls surface maxit as an option to the user?
      if(mod$nint){
        fit = VGAM::vglm( mod$DependentVar ~ as.matrix(mod$Regressor[,2:(1+mod$nreg)]), genpoisson2, maxit=60)
      }else{
        fit = VGAM::vglm( mod$DependentVar ~  0 + as.matrix(mod$Regressor[,1:mod$nreg]), genpoisson2(zero=0), maxit=60,)
      }
      # save linear predictor coefficients
      est[1:(mod$nMargParms-1)]  = as.numeric(coef(fit, matrix = TRUE)[,1])

      # save dispersion coefficient
      est[mod$nMargParms] = loglink(as.numeric(coef(fit, matrix = TRUE)[1,2]),inverse = TRUE)
    }
  }

  #-----Binomial case
  if(mod$CountDist=="Binomial"){
    if(mod$nreg==0){
      xbar = mean(mod$DependentVar)
      # Method of Moments for Binomial E(X)=rp, where  r = ntrials
      pEst = xbar/mod$ntrials
      est[1] = pEst
    }else{
      #glmBinomial               = glm(cbind(mod$DependentVar,mod$ntrials-mod$DependentVar) ~ mod$Regressor[,2:(mod$nreg+1)] , family = 'binomial')
      if(mod$nint){
        glmBinomial = glm(cbind(mod$DependentVar,mod$ntrials-mod$DependentVar) ~ as.matrix(mod$Regressor[,2:(mod$nreg+1)]) , family = 'binomial')
      }else{
        glmBinomial = glm(cbind(mod$DependentVar,mod$ntrials-mod$DependentVar) ~  0 + as.matrix(mod$Regressor[,1:mod$nreg]) , family = 'binomial')

      }
      est[1:mod$nMargParms] = as.numeric(glmBinomial$coef)
    }
  }

  #-----Zero Inflation Poisson
  if(mod$CountDist=="ZIP"){
    if(mod$nreg==0){
      # pmle for marginal parameters
      ZIP_PMLE <- poisson.zihmle(mod$DependentVar, type = c("zi"), lowerbound = 0.01, upperbound = 10000)

      lEst = ZIP_PMLE[1]
      pEst = ZIP_PMLE[2]
      est[1:2] = c(lEst, pEst)

    }else{
      #zeroinfl_reg <- zeroinfl( mod$DependentVar~ mod$Regressor[,2:(mod$nreg+1)] | 1,)
      #zeroinfl_reg = vglm( mod$DependentVar~ mod$Regressor[,2:(mod$nreg+1)], zipoisson(zero=1))

      if(mod$nint){
        zeroinfl_reg = vglm( mod$DependentVar ~ as.matrix(mod$Regressor[,2:(mod$nreg+1)]), zipoisson(zero=1))
      }else{
        zeroinfl_reg = vglm( mod$DependentVar ~ 0 + as.matrix(mod$Regressor[,1:mod$nreg]), zipoisson(zero=0))
      }

      est[1:(mod$nMargParms-1)] = as.numeric(coef(zeroinfl_reg))[2:(mod$nMargParms)]
      est[mod$nMargParms] = plogis(as.numeric(coef(zeroinfl_reg))[1])
    }
  }

  #------------ARMA Initial Estimates
  # Transform (1) in the JASA paper to retrieve the "observed" latent series and fit an ARMA
  if(mod$nreg==0){

    Cxt = mod$mycdf(mod$DependentVar,est[1:mod$nMargParms])
    # if Cxt = 1, I will need to deal with this somehow. In the Binomial case it seems very likely to get 1
    # but this can also happen for other distributions. So far, in the initial estimation I have only seen the issue
    # taking place in the Binomial case, however, I have come across this issue on the Particle filter estimation
    # for misspecified models. For now I will simply set
    if (mod$CountDist=="Binomial"){
      Cxt[Cxt==1] = 1-10^(-16)
      Cxt[Cxt==0] = 0+10^(-16)
    }
    # I am doing it for the binomial only so that I will get an error if it happens for another case.
    if(max(mod$ARMAModel)>0) armafit = itsmr::arma(qnorm(Cxt),mod$nAR,mod$nMA)
    if(mod$nAR>0) est[(mod$nMargParms+1):(mod$nMargParms+mod$nAR)]                    = armafit$phi
    if(mod$nMA>0) est[(1+mod$nMargParms+mod$nAR):(mod$nMargParms+sum(mod$ARMAModel))] = armafit$theta
  }else{
    # put the parameters in appropriate format
    Params = RetrieveParameters(est,mod)

    # see comment above regarding Cxt=1 and Cxt = 0
    Cxt = mod$mycdf(mod$DependentVar,Params$ConstMargParm, Params$DynamMargParm)
    if (mod$CountDist=="Binomial" || mod$CountDist=="Mixed Poisson" ){
      Cxt[Cxt==1] = 1-10^(-16)
      Cxt[Cxt==0] = 0+10^(-16)
    }
    # Transform (1) in the JASA paper to retrieve the "observed" latent series and fit an ARMA
    if(mod$nAR || mod$nMA) ARMAFit = itsmr::arma(qnorm(Cxt),mod$nAR,mod$nMA)
    if(mod$nAR) est[(mod$nMargParms+1):(mod$nMargParms+mod$nAR)]                    = ARMAFit$phi
    if(mod$nMA) est[(1+mod$nMargParms+mod$nAR):(mod$nMargParms+sum(mod$ARMAModel))] = ARMAFit$theta
  }

  # add the parmnames on theta fix me: does this affect performance?
  names(est) = mod$parmnames


  return(est)
}

#' Innovations Algorithm for Time Series Prediction
#'
#' Applies the Innovations Algorithm to compute the optimal one-step-ahead predictors
#' and associated prediction error variances based on the autocovariance sequence
#' of a stationary time series. The algorithm uses the ARMA parameters provided in
#' \code{Parms} and the autocovariance vector \code{gamma} to generate recursive
#' estimates of innovation coefficients and variances.
#'
#' @param Parms A list containing the model parameters:
#'   \itemize{
#'     \item \code{AR}: Vector of autoregressive (AR) coefficients.
#'     \item \code{MA}: Vector of moving average (MA) coefficients.
#'   }
#' @param gamma Numeric vector. The autocovariance sequence of the observed time series.
#'   Typically length \eqn{N}, where \eqn{N} is the time series length.
#' @param mod A list returned by \code{\link{ModelScheme}}, containing model meta-data
#'   such as \code{nAR}, \code{nMA}, and \code{maxdiff}. Used to define stopping
#'   conditions for the recursion based on model order and numerical precision.
#'
#' @return A list with the following elements:
#' \describe{
#'   \item{\code{n}}{Number of recursion steps performed (effective model order).}
#'   \item{\code{thetas}}{List of innovation coefficients (one per step). Each element is a vector of length \code{q}.}
#'   \item{\code{v}}{Vector of innovation variances at each step (normalized by \code{v[1]}).}
#' }
#'
#' @details
#' This implementation handles:
#' \itemize{
#'   \item AR-only models
#'   \item MA-only models
#'   \item Combined ARMA models
#' }
#' The recursion is terminated based on a fixed tolerance (\code{mod$maxdiff}) for variance convergence in MA models,
#' or after a fixed number of steps in AR-only models. The \code{kappa(i,j)} function computes the autocovariance
#' terms needed for recursion using model coefficients and the autocovariance vector \code{gamma}.
#'
#' @examples
#' # ARMA(1,1) example
#' gamma <- ARMAacf(ar = 0.5, ma = 0.4, lag.max = 20)
#' mod <- list(nAR = 1, nMA = 1, maxdiff = 1e-8)
#' Parms <- list(AR = 0.5, MA = 0.4)
#' result <- InnovAlg(Parms, gamma, mod)
#' str(result)
#'
#' @export
InnovAlg = function(Parms,gamma, mod) {
  # adapt the Innovation.algorithm from ITSMR
  # check me: explain in the doc the exact modifications
  # Compute autocovariance kappa(i,j) per equation (3.3.3)

  # Optimized for i >= j and j > 0

  kappa = function(i,j) {
    if (j > m)
      return(sum(theta_r[1:(q+1)] * theta_r[(i-j+1):(i-j+q+1)]))
    else if (i > 2*m)
      return(0)
    else if (i > m)
      return((gamma[i-j+1] - sum(phi * gamma[abs(seq(1-i+j,p-i+j))+1]))/sigma2)
    else
      return(gamma[i-j+1]/sigma2)
  }

  phi     = Parms$AR
  sigma2  = 1
  N       = length(gamma)
  theta_r = c(1,Parms$MA,numeric(N))

  # Innovations algorithm

  p = ifelse(is.null(Parms$AR),0,length(Parms$AR))
  q = ifelse(is.null(Parms$MA),0,length(Parms$MA))
  m = max(p,q)

  Theta   = list()
  v       = rep(NA,N+1)
  v[1]    = kappa(1,1)
  StopCondition = FALSE
  n       = 1


  while(!StopCondition && n<N ) {
    Theta[[n]] <- rep(0,q)
    Theta[[n]][n] = kappa(n+1,1)/v[1]
    if(n>q && mod$nAR==0) Theta[[n]][n]= 0
    if(n>1){
      for (k in 1:(n-1)) {
        js <- 0:(k-1)
        Theta[[n]][n-k] <- (kappa(n+1,k+1) - sum(Theta[[k]][k-js]*Theta[[n]][n-js]*v[js+1])) / v[k+1]
      }
    }
    js     = 0:(n-1)
    v[n+1] = kappa(n+1,n+1) - sum(Theta[[n]][n-js]^2*v[js+1])
    if(mod$nAR==0 && mod$nMA>0) StopCondition = (abs(v[n+1]-v[n])< mod$maxdiff)
    if(mod$nAR>0 && mod$nMA==0) StopCondition = (n>3*m)
    n      = n+1
  }
  v = v/v[1]

  StopCondition = (abs(v[n+1]-v[n])< mod$maxdiff)


  return(list(n=n-1,thetas=lapply(Theta[ ][1:(n-1)], function(x) {x[1:q]}),v=v[1:(n-1)]))
}

# simulate from our model
sim_lgc_old = function(n, CountDist, MargParm, ARParm, MAParm, Regressor=NULL, Intercept=NULL){

  # Generate latent Gaussian model
  z  =arima.sim(model = list( ar = ARParm, ma=MAParm  ), n = n)
  z = z/sd(z) # standardize the data

  # add a column of ones in the Regressors if Intercept is present
  if (!is.null(Intercept) && Intercept==TRUE && sum(as.matrix(Regressor)[,1])!=n){
    Regressor = as.data.frame(cbind(rep(1,n),Regressor))
    names(Regressor)[1] = "Intercept"
  }


  # number of regressors
  nreg = ifelse(is.null(Regressor), 0, dim(Regressor)[2]-as.numeric(Intercept))

  if(nreg==0){
    # retrieve marginal inverse cdf
    myinvcdf = switch(CountDist,
                      "Poisson"               = qpois,
                      "Negative Binomial"     = function(x, theta){ qnbinom (x, theta[1], 1-theta[2]) },
                      "Generalized Poisson"   = function(x, theta){ qGpois  (x, theta[1], theta[2])},
                      "Generalized Poisson 2" = function(x, theta){ qGpois  (x, theta[2], theta[1])},
                      "Binomial"              = qbinom,
                      "Mixed Poisson"         = function(x, theta){qmixpois1(x, theta[1], theta[2], theta[3])},
                      "ZIP"                   = function(x, theta){ qzipois(x, theta[1], theta[2]) },
                      stop("The specified distribution is not supported.")
    )

    # get the final counts
    x = myinvcdf(pnorm(z), MargParm)

  }else{
    # retrieve inverse count cdf
    myinvcdf = switch(CountDist,
                      "Poisson"               = function(x, ConstMargParm, DynamMargParm){ qpois   (x, DynamMargParm)},
                      "Negative Binomial"     = function(x, ConstMargParm, DynamMargParm){ qnbinom (x, ConstMargParm, 1-DynamMargParm)},
                      "Generalized Poisson"   = function(x, ConstMargParm, DynamMargParm){ qGpois  (x, ConstMargParm, DynamMargParm)},
                      "Generalized Poisson 2" = function(x, ConstMargParm, DynamMargParm){ qgenpois2  (x, DynamMargParm, ConstMargParm)},
                      "Binomial"              = function(x, ConstMargParm, DynamMargParm){ qbinom  (x, ConstMargParm, DynamMargParm)},
                      "Mixed Poisson"         = function(x, ConstMargParm, DynamMargParm){qmixpois1(x, DynamMargParm[,1], DynamMargParm[,2],  ConstMargParm)},
                      "ZIP"                   = function(x, ConstMargParm, DynamMargParm){ qzipois (x, DynamMargParm[,1], DynamMargParm[,2]) },
                      stop("The specified distribution is not supported.")
    )

    # regression parameters
    beta  = MargParm[1:(nreg+1)]
    m     = exp(as.matrix(Regressor)%*%beta)

    if(CountDist == "Poisson" && nreg>0){
      ConstMargParm  = NULL
      DynamMargParm  = m
    }

    if(CountDist == "Negative Binomial" && nreg>0){
      ConstMargParm  = 1/MargParm[nreg+2]
      DynamMargParm  = MargParm[nreg+2]*m/(1+MargParm[nreg+2]*m)
    }

    if(CountDist == "Generalized Poisson" && nreg>0){
      ConstMargParm  = MargParm[nreg+2]
      DynamMargParm  = m
    }


    if(CountDist == "Generalized Poisson 2" && nreg>0){
      ConstMargParm  = MargParm[nreg+2]
      DynamMargParm  = m
    }

    if(CountDist == "Binomial" && nreg>0){
      # ConstMargParm  = MargParm[nreg+2]
      DynamMargParm  = m/(1+m)
    }

    if(CountDist == "Mixed Poisson" && nreg>0){
      ConstMargParm  = c(MargParm[nreg*2+3], 1 - MargParm[nreg*2+3])
      DynamMargParm  = cbind(exp(as.matrix(Regressor)%*%MargParm[1:(nreg+1)]),
                             exp(as.matrix(Regressor)%*%MargParm[(nreg+2):(nreg*2+2)]))
    }

    if(CountDist == "ZIP" && nreg>0){
      ConstMargParm  = NULL
      DynamMargParm  = cbind(exp(as.matrix(Regressor)%*%MargParm[1:(nreg+1)]),
                             1/(1+exp(-as.matrix(Regressor)%*%MargParm[(nreg+2):(nreg*2+2)])))
    }

    # get the final counts
    x = myinvcdf(pnorm(z), ConstMargParm, DynamMargParm)
  }
  return(as.numeric(x))
}

#' Simulate Count Time Series from Specified Model
#'
#' Simulates a univariate count time series of length \code{n} from a specified model
#' defined by a marginal distribution, ARMA dependence structure, and optional regression
#' covariates. Supports various count distributions and includes options for an intercept
#' and binomial trial counts.
#'
#' @param n Integer. Length of the time series to simulate.
#' @param CountDist Character. Name of the marginal count distribution to simulate from.
#'   Supported values include \code{"Poisson"}, \code{"Negative Binomial"}, \code{"ZIP"},
#'   \code{"Generalized Poisson"}, \code{"Mixed Poisson"}, \code{"Binomial"}, etc.
#' @param MargParm Numeric vector. Parameters of the marginal distribution (e.g., mean, dispersion).
#' @param ARParm Numeric vector. Autoregressive (AR) parameters. Can be empty or \code{NULL} for no AR component.
#' @param MAParm Numeric vector. Moving average (MA) parameters. Can be empty or \code{NULL} for no MA component.
#' @param Regressor Optional matrix or data frame of covariates. Used for dynamic regression models.
#' @param Intercept Logical or numeric. Whether to include an intercept term. If numeric, should be 0 or 1.
#' @param ntrials Integer. Number of trials for the Binomial model (required if \code{CountDist == "Binomial"}).
#' @param ... Additional arguments (currently unused).
#'
#' @return A numeric vector of length \code{n}, containing the simulated count time series.
#'
#' @details
#' The simulation is based on a latent Gaussian copula framework, where the marginal distribution
#' is applied to transformed latent variables with dependence induced by an ARMA process.
#'
#' The function accommodates:
#' \itemize{
#'   \item Purely marginal models (no AR/MA)
#'   \item Dynamic regression models with covariates
#'   \item ARMA-dependent latent structure for autocorrelation
#' }
#' Be sure that \code{MargParm} and other arguments match the chosen \code{CountDist}.
#'
#' @examples
#' # Simulate Poisson-ARMA(1,1) series
#' sim <- sim_lgc(
#'   n = 100,
#'   CountDist = "Poisson",
#'   MargParm = 3,
#'   ARParm = 0.5,
#'   MAParm = -0.3
#' )
#' ts.plot(sim)
#'
#' @export
sim_lgc = function(n, CountDist, MargParm, ARParm, MAParm, Regressor=NULL, Intercept=NULL, ntrials=NULL,...){

  # combine all parameters in on vector
  TrueParam = c(MargParm,ARParm,MAParm)

  # set ARMAModel orders
  ARMAModel = c(length(ARParm), length(MAParm))

  # add a column of ones in the Regressors if Intercept is present
  if (!is.null(Intercept) && Intercept==TRUE && sum(as.matrix(Regressor)[,1])!=n){
    Regressor = as.data.frame(cbind(rep(1,n),Regressor))
    names(Regressor)[1] = "Intercept"
  }

  # parse the model information - needed for distribution functions
  mod = ModelScheme(SampleSize = n,
                     CountDist = CountDist,
                     ARMAModel = ARMAModel,
                     TrueParam = TrueParam,
                     Regressor = Regressor,
                     Intercept = Intercept,
                          Task = "Synthesis",
                       ntrials = ntrials)

  # reorganize the parameters in expected format
  Parms = RetrieveParameters(TrueParam,mod)

  # Generate latent Gaussian model
  z  =arima.sim(model = list( ar = ARParm, ma=MAParm), n = n)
  z = z/sd(z) # standardize the data

  # apply the inverse count cdf - check me: in Binomial case ntrials comes as global
  if(mod$nreg==0){
    x = mod$myinvcdf(pnorm(z), Parms$MargParms)
  }else{
    x = mod$myinvcdf(pnorm(z), Parms$ConstMargParm, Parms$DynamMargParm)
  }
  # FIX ME: Should I be returning ts objects?
  return(as.numeric(x))
}


#' Compute AIC, BIC, and AICc for Model Evaluation
#'
#' Computes the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC),
#' and corrected AIC (AICc) for a fitted model, based on the log-likelihood, sample size,
#' and number of parameters. Adjustments are made depending on the estimation method.
#'
#' @param loglik Numeric. The log-likelihood value of the fitted model.
#' @param mod A list containing model-related metadata, typically created by
#'   \code{\link{ModelScheme}}. It must include:
#'   \itemize{
#'     \item \code{nparms}: Number of estimated parameters.
#'     \item \code{n}: Sample size.
#'     \item \code{EstMethod}: Estimation method used ("GL" or "PFR").
#'   }
#'
#' @return A numeric vector of length 3 containing the values:
#' \describe{
#'   \item{\code{AIC}}{Akaike Information Criterion}
#'   \item{\code{BIC}}{Bayesian Information Criterion}
#'   \item{\code{AICc}}{Corrected Akaike Information Criterion}
#' }
#'
#' @details
#' For models estimated via Gaussian likelihood (\code{EstMethod = "GL"}), the log-likelihood
#' is adjusted by subtracting \code{n/2 * log(2π)} to convert it to the classic likelihood form
#' used in AIC/BIC computation. For particle filter-based estimation methods (\code{"PFR"}),
#' no correction is applied.
#'
#' This function is used to evaluate and compare model fits, especially when selecting among
#' candidate models of varying complexity.
#'
#' @examples
#' mod <- list(nparms = 5, n = 100, EstMethod = "PFR")
#' loglik <- -123.45
#' Criteria.lgc(loglik, mod)
#'
#' @export
Criteria.lgc = function(loglik, mod){

  # initial versions of the function included Gaussial Likelihood as estimation method
  if(mod$EstMethod!="GL"){
    l1 = -loglik
  }else{
    l1 = -loglik  - mod$n/2*log(2*pi)
  }

  AIC = 2*mod$nparms - 2*l1
  BIC = log(mod$n)*mod$nparms - 2*l1
  AICc = AIC + (2*mod$nparms^2 + 2*mod$nparms)/(mod$n-mod$nparms-1)

  AllCriteria = c(AIC, BIC, AICc)
}

#' @title Log-Likelihood for lgc Model
#' @description Returns the log-likelihood value stored in a fitted `lgc` model object.
#' @param object An object of class `lgc`.
#' @param ... Additional arguments (currently unused).
#' @return Numeric. Log-likelihood value.
#' @exportS3Method logLik lgc
logLik.lgc = function(object,...){
  return(object$FitStatistics[1])
}

#' @title AIC for lgc Model
#' @description Returns the AIC value for a fitted `lgc` model.
#' @param object An object of class `lgc`.
#' @param ... Additional arguments (currently unused).
#' @return Numeric. AIC value.
#' @exportS3Method AIC lgc
AIC.lgc = function(object,...){
  return(object$FitStatistics[2])
}

BIC <- function(object, ...) UseMethod("BIC")

#' @title BIC for lgc Model
#' @description Returns the BIC value for a fitted `lgc` model.
#' @param object An object of class `lgc`.
#' @param ... Additional arguments (currently unused).
#' @return Numeric. BIC value.
#' @exportS3Method BIC lgc
BIC.lgc = function(object,...){
  return(object$FitStatistics[3])
}

se <- function(object, ...) UseMethod("se")

#' @title Standard Errors for lgc Model
#' @description Returns the standard errors from a fitted `lgc` model.
#' @param object An object of class `lgc`.
#' @param ... Additional arguments (currently unused).
#' @return Matrix of standard errors.
#' @exportS3Method se lgc
se.lgc = function(object,...){
  return(object$StdErrors)
}

coefficients <- function(object, ...) UseMethod("coefficients")

#' @title Coefficients for lgc Model
#' @description Returns the estimated coefficients from a fitted `lgc` model.
#' @param object An object of class `lgc`.
#' @param ... Additional arguments (currently unused).
#' @return Matrix of parameter estimates.
#' @exportS3Method coefficients lgc
coefficients.lgc = function(object,...){
  return(object$ParamEstimates)
}

model <- function(object, ...) UseMethod("model")

#' @title Model Summary for lgc Object
#' @description Returns a data frame summarizing the distribution and ARMA model structure.
#' @param object An object of class `lgc`.
#' @param ... Additional arguments (currently unused).
#' @return A data frame with two columns: distribution and model type.
#' @exportS3Method model lgc
model.lgc = function(object,...){
  # if ((object$ARMAModel[1]>0) &&  (object$ARMAModel[2]>0)){
  #   ARMAModel = sprintf("ARMA(%.0f, %.0f)",object$ARMAModel[1], object$ARMAModel[2])
  # }
  # if ((object$ARMAModel[1]>0) &&  (object$ARMAModel[2]==0)){
  #   ARMAModel = sprintf("AR(%.0f)",object$ARMAModel[1])
  # }
  # if ((object$ARMAModel[1]==0) &&  (object$ARMAModel[2]>0)){
  #   ARMAModel = sprintf("MA(%.0f)",object$ARMAModel[2])
  # }
  # if ((object$ARMAModel[1]==0) &&  (object$ARMAModel[2]==0)){
  #   ARMAModel = "White Noise"
  # }
  #
  # a = data.frame(object$CountDist, ARMAModel)
  # names(a) = c("Distribution", "Model")
  # return(a)
  return(object$Model)
}



#' @title Extract Residuals from an lgc Model
#' @description Computes residuals from a fitted \code{lgc} model using the
#'   ARMA structure specified in the model.
#' @param object An object of class \code{lgc}.
#' @param ... Additional arguments (currently unused).
#' @return A numeric vector of residuals of length \code{nobs(object)}.
#' @examples
#' \dontrun{
#'   residuals(mylgc)
#' }
#' @exportS3Method residuals lgc
residuals.lgc <- function(object, ...) {
  if (!inherits(object, "lgc")) stop("Object must be of class 'lgc'")
  res <- ComputeResiduals(object)
  res
}



#' @title Summarize an lgc Model
#' @description Provides a summary of a fitted \code{lgc} model, including
#'   parameter estimates and fit statistics.
#' @param object An object of class \code{lgc}.
#' @param ... Additional arguments (currently unused).
#' @return Invisibly returns the original \code{lgc} object.
#' @examples
#' \dontrun{
#'   summary(mylgc)
#' }
#' @exportS3Method summary lgc
summary.lgc <- function(object, ...) {
  cat("Model:\n")
  print(object$Model)

  cat("\nParameter Estimates:\n")
  print(object$ParamEstimates)

  if (!is.null(object$SE)) {
    cat("\nStandard Errors:\n")
    print(object$SE)
  }

  cat("\nFit Statistics:\n")
  print(object$FitStatistics)

  invisible(object)
}

# print <- function(object, ...) UseMethod("print")

#' @title Print an lgc Model
#' @description Nicely formats and prints a fitted \code{lgc} model object.
#' @param x An object of class \code{lgc}.
#' @param ... Additional arguments (currently unused).
#' @return Invisibly returns the input object.
#' @examples
#' \dontrun{
#'   print(mylgc)
#' }
#' @exportS3Method print lgc
print.lgc <- function(x, ...) {
  cat("Latent Gaussian Count Model\n")
  cat("Model:", x$Model, "\n")
  cat("Log-likelihood:", x$FitStatistics["loglik"], "\n")
  cat("Parameters:\n")
  print(x$ParamEstimates)
  invisible(x)
}


#' @title Number of Observations in lgc Model
#' @description Returns the number of observations used in fitting the model.
#' @param object An object of class \code{lgc}.
#' @param ... Additional arguments (currently unused).
#' @return An integer, the number of observations.
#' @exportS3Method stats::nobs lgc
nobs.lgc <- function(object, ...) {
  return(as.integer(object$SampleSize))
}


#' Compute Truncation Limits for Latent Gaussian Count Models
#'
#' Computes the lower and upper truncation limits used in the sampling of the latent variable
#' in the particle filter algorithm. Corresponds to equation (19) in the JASA paper.
#'
#' @param mod A list containing model specification and data, typically created by \code{\link{ModelScheme}}.
#' @param Parms A list of model parameters retrieved via \code{\link{RetrieveParameters}}.
#' @param t Integer. Current time point.
#' @param Zhat Numeric vector of predicted latent values up to time \code{t}.
#' @param Rt Numeric vector of innovation standard deviations from the innovations algorithm.
#'
#' @return A list with elements \code{a} and \code{b} representing the lower and upper limits
#' for truncated normal sampling.
#' @keywords internal
#' @seealso \code{\link{SampleTruncNormParticles}}
#' @export
ComputeLimits = function(mod, Parms, t, Zhat, Rt){

  # a and b are the arguments in the two normal cdfs in the 4th line in equation (19) in JASA paper
  Lim = list()
  # fix me: this will be ok for AR or MA models but for ARMA? is it p+q instead of max(p,q)
  # fix me: why do I have t(Parms$MargParms)?
  index = min(t, max(mod$ARMAModel))

  # add the following for White Noise models
  if(max(mod$ARMAModel)==0){index=1}

  if(mod$nreg==0){
    Lim$a = as.numeric((qnorm(mod$mycdf(mod$DependentVar[t]-1,t(Parms$MargParms)),0,1)) - Zhat)/(Rt[index])
    Lim$b = as.numeric((qnorm(mod$mycdf(mod$DependentVar[t],t(Parms$MargParms)),0,1)) - Zhat)/Rt[index]
  }else{
    Lim$a = as.numeric((qnorm(mod$mycdf(mod$DependentVar[t]-1,Parms$ConstMargParm, Parms$DynamMargParm[t,]),0,1)) - Zhat)/Rt[index]
    Lim$b = as.numeric((qnorm(mod$mycdf(mod$DependentVar[t],Parms$ConstMargParm, Parms$DynamMargParm[t,]),0,1)) - Zhat)/Rt[index]
  }

  return(Lim)
}

#' Sample Truncated Normal Particles
#'
#' Generates samples from a truncated normal distribution using inverse transform sampling,
#' as required by the particle filter for latent Gaussian models.
#'
#' @param mod Model list as returned by \code{\link{ModelScheme}}.
#' @param Limit A list of truncation limits with elements \code{a} and \code{b}, typically
#' returned by \code{\link{ComputeLimits}}.
#' @param t Integer. Current time index.
#' @param Zhat Numeric vector of predicted latent values up to time \code{t}.
#' @param Rt Numeric vector of innovation standard deviations from the innovations algorithm.
#'
#' @return A numeric vector of sampled latent variables.
#' @keywords internal
#' @export
SampleTruncNormParticles = function(mod, Limit, t, Zhat, Rt){
  # relation (21) in JASA paper and the inverse transform method
  # check me: this can be improved?
  index = min(t, max(mod$ARMAModel))
  if(max(mod$ARMAModel)==0){index=1}
  z = qnorm(runif(length(Limit$a),0,1)*(pnorm(Limit$b,0,1)-pnorm(Limit$a,0,1))+pnorm(Limit$a,0,1),0,1)*Rt[index] + Zhat
  return(z)
}

#' Compute Filtered Latent Value at Time t
#'
#' Computes the conditional mean of the latent variable at time \code{t} using the innovations
#' algorithm output.
#'
#' @param mod Model list as returned by \code{\link{ModelScheme}}.
#' @param IA A list returned by \code{\link{InnovAlg}}, containing innovation variances and
#' theta coefficients.
#' @param Z Matrix of latent particle values.
#' @param Zhat Matrix of predicted latent means.
#' @param t Integer. Time point at which to compute the latent mean.
#' @param Parms List of model parameters from \code{\link{RetrieveParameters}}.
#'
#' @return A numeric vector with the filtered latent value at time \code{t}.
#' @keywords internal
#' @export
ComputeZhat_t = function(mod, IA, Z, Zhat,t, Parms){

  Theta    = IA$thetas
  nTheta   = length(IA$thetas)
  Theta_n  = Theta[[nTheta]]

  m = max(mod$ARMAModel)
  p = mod$ARMAModel[1]
  q = mod$ARMAModel[2]

  if(m>1 && t<=m) Zhat_t = Theta[[t-1]][1:(t-1)]%*%(Z[(t-1):1,]-Zhat[(t-1):1,])

  if(t>m && t<=nTheta){
    A = B= 0
    if(!is.null(Parms$AR)) A = Parms$AR%*%Z[(t-1):(t-p),]
    if(!is.null(Parms$MA)) B = Theta[[t-1]][1:q]%*%(Z[(t-1):(t-q),]-Zhat[(t-1):(t-q),])
    Zhat_t = A + B
  }
  if(t>nTheta){
    A = B = 0
    if(!is.null(Parms$AR)) A = Parms$AR%*%Z[(t-1):(t-p),]
    if(!is.null(Parms$MA)) B = Theta_n[1:q]%*%(Z[(t-1):(t-q),]-Zhat[(t-1):(t-q),])
    Zhat_t = A + B
  }

  return(Zhat_t)
}

#' Compute Particle Weights
#'
#' Computes the importance weights of the particles at time \code{t}, based on the
#' truncation limits.
#'
#' @param mod Model list as returned by \code{\link{ModelScheme}}.
#' @param Limit A list of truncation limits (from \code{\link{ComputeLimits}}).
#' @param t Integer. Current time index.
#' @param PreviousWeights Numeric vector of weights at time \code{t-1}.
#'
#' @return A numeric vector of updated particle weights at time \code{t}.
#' @keywords internal
#' @export
ComputeWeights = function(mod, Limit, t, PreviousWeights){
  # equation (21) in JASA paper
  # update weights
  if(t<=max(mod$ARMAModel)){
    NewWeights = PreviousWeights*(pnorm(Limit$b,0,1) - pnorm(Limit$a,0,1))
  }else{ # fix me: if I add the wgh[t-1,] below as I should the weights become small?
    NewWeights = (pnorm(Limit$b,0,1) - pnorm(Limit$a,0,1))
  }

  return(NewWeights)
}

#' Resample Particles Using Effective Sample Size (ESS) Criterion
#'
#' Performs resampling of particles when the effective sample size falls below a
#' threshold defined by the model. Follows relation (26) in the JASA paper.
#'
#' @param mod Model list as returned by \code{\link{ModelScheme}}.
#' @param wgh Matrix of particle weights for all time points.
#' @param t Integer. Current time index.
#' @param Znew Vector of new particle samples to be resampled.
#'
#' @return A resampled vector of particles of length equal to \code{mod$ParticleNumber}.
#' @keywords internal
#' @export
ResampleParticles = function(mod, wgh, t, Znew){

  # relation (26) in JASA paper and following step
  # compute normalized weights
  wghn = wgh[t,]/sum(wgh[t,])

  # effective sample size
  ESS = 1/sum(wghn^2)

  if(ESS<mod$epsilon*mod$ParticleNumber){
    ind = rmultinom(1,mod$ParticleNumber,wghn)
    Znew = rep(Znew,ind)
  }
  return(Znew)
}

#' Extract model Parameters from Flat Vector
#'
#' Decomposes a flat parameter vector \code{theta} into structured components
#' according to the model specification provided in \code{mod}. This includes
#' marginal distribution parameters, autoregressive (AR) and moving average (MA)
#' components, and dynamic GLM-type parameters when covariates are present.
#'
#' @param theta Numeric vector. The complete parameter vector, typically as passed
#'   to or from an optimizer.
#' @param mod A list containing the model specification, as produced by
#'   \code{\link{ModelScheme}}. It must contain fields such as:
#'   \itemize{
#'     \item \code{MargParmIndices}: Indices of marginal parameters in \code{theta}
#'     \item \code{CountDist}: Name of the count distribution
#'     \item \code{Regressor}: Covariate matrix (if applicable)
#'     \item \code{nreg}, \code{nint}, \code{nMargParms}: Model structure details
#'     \item \code{ARMAModel}: Integer vector specifying AR and MA orders
#'   }
#'
#' @return A named list with the following components:
#' \describe{
#'   \item{\code{MargParms}}{Marginal distribution parameters (from \code{theta}).}
#'   \item{\code{ConstMargParm}}{Constant component(s) of the marginal distribution (if applicable).}
#'   \item{\code{DynamMargParm}}{Dynamic component(s), such as covariate effects (if applicable).}
#'   \item{\code{AR}}{Autoregressive coefficients (if present).}
#'   \item{\code{MA}}{Moving average coefficients (if present).}
#' }
#'
#' @details
#' This function supports several count distrib
RetrieveParameters = function(theta,mod){

  Parms =  vector(mode = "list", length = 5)


  names(Parms) = c("MargParms", "ConstMargParm", "DynamMargParm", "AR", "MA")

  # marginal parameters
  Parms$MargParms      = theta[mod$MargParmIndices]

  # regressor parameters
  if(mod$nreg>0){
    beta  = Parms$MargParms[1:(mod$nreg+mod$nint)]
    m     = exp(as.matrix(mod$Regressor)%*%beta)
  }

  # GLM type parameters
  if(mod$CountDist == "Negative Binomial" && mod$nreg>0){
    Parms$ConstMargParm  = 1/Parms$MargParms[mod$nreg+mod$nint+1]
    Parms$DynamMargParm  = Parms$MargParms[mod$nreg+mod$nint+1]*m/(1+Parms$MargParms[mod$nreg+mod$nint+1]*m)
  }

  if(mod$CountDist == "Generalized Poisson" && mod$nreg>0){
    Parms$ConstMargParm  = Parms$MargParms[mod$nreg+mod$nint+1]
    Parms$DynamMargParm  = m
  }

  if(mod$CountDist == "Generalized Poisson 2" && mod$nreg>0){
    Parms$ConstMargParm  = Parms$MargParms[mod$nreg+mod$nint+1]
    Parms$DynamMargParm  = m
  }

  if(mod$CountDist == "Poisson" && mod$nreg>0){
    Parms$ConstMargParm  = NULL
    Parms$DynamMargParm  = m
  }

  if(mod$CountDist == "Binomial" && mod$nreg>0){
    Parms$ConstMargParm  = NULL
    Parms$DynamMargParm  = m/(1+m)
  }

  if(mod$CountDist == "Mixed Poisson" && mod$nreg>0){
    Parms$ConstMargParm  = c(Parms$MargParms[2*(mod$nreg+mod$nint)+1])
    Parms$DynamMargParm  = cbind(m, exp(as.matrix(mod$Regressor)%*%Parms$MargParms[(mod$nreg+mod$nint+1):((mod$nreg+mod$nint)*2)]))
  }

  if(mod$CountDist == "ZIP" && mod$nreg>0){
    Parms$ConstMargParm  = Parms$MargParms[mod$nreg+mod$nint+1]
    Parms$DynamMargParm  = m
  }


  # Parms$DynamMargParm = as.matrix(Parms$DynamMargParm)

  # Parms$AR = NULL
  if(mod$ARMAModel[1]>0) Parms$AR = theta[(mod$nMargParms+1):(mod$nMargParms + mod$ARMAModel[1])]

  # Parms$MA = NULL
  if(mod$ARMAModel[2]>0) Parms$MA = theta[(mod$nMargParms+mod$ARMAModel[1]+1) :
                                            (mod$nMargParms + mod$ARMAModel[1] + mod$ARMAModel[2]) ]


  return(Parms)
}

# Mixed Poisson inverse cdf - this will not allow for vector lambda
# qmixpois1 = function(y, lam1, lam2, p){
#   yl = length(y)
#   x  = rep(0,yl)
#   for (n in 1:yl){
#     while(pmixpois(x[n], lam1, lam2,p) <= y[n]){ # R qpois would use <y; this choice makes the function right-continuous; this does not really matter for our model
#       x[n] = x[n]+1
#     }
#   }
#   return(x)
# }

#' Computes the quantiles (inverse CDF) of the two-component mixed Poisson distribution
#' using a brute-force loop-based method.
#'
#' @param y Numeric vector of quantile probabilities (between 0 and 1).
#' @param lam1 Numeric or vector. Mean(s) of the first Poisson component.
#' @param lam2 Numeric or vector. Mean(s) of the second Poisson component.
#' @param p Numeric between 0 and 1. Mixing probability for the first component.
#'
#' @return A numeric vector of quantiles corresponding to the input probabilities.
#'
#' @details
#' For each value in \code{y}, the function finds the smallest integer \code{x} such that
#' \code{pmixpois1(x, lam1, lam2, p) >= y}.
#'
#' @examples
#' qmixpois1(c(0.25, 0.5, 0.75), lam1 = 2, lam2 = 5, p = 0.6)
#'
#' @export
qmixpois1 = function(y, lam1, lam2, p){
  yl = length(y)  # Length of the input vector y
  x  = rep(0, yl) # Initialize a vector of zeros to store results

  for (n in 1:yl) {
    lambda1 = ifelse(length(lam1) > 1, lam1[n], lam1)  # Use indexed lambda1 or constant
    lambda2 = ifelse(length(lam2) > 1, lam2[n], lam2)  # Use indexed lambda2 or constant

    # Increment x[n] until the CDF is greater than y[n]
    while (pmixpois1(x[n], lambda1, lambda2, p) <= y[n]) {
      x[n] = x[n] + 1
    }
  }

  return(x)
}

#' Vectorized Quantile Function for Mixed Poisson Distribution
#'
#' Computes quantiles of the mixed Poisson distribution using a vectorized approach
#' with \code{mapply()}.
#'
#' @param p Numeric vector of probabilities (between 0 and 1).
#' @param lam1 Numeric or vector. Mean(s) of the first Poisson component.
#' @param lam2 Numeric or vector. Mean(s) of the second Poisson component.
#' @param prob Numeric between 0 and 1. Mixing probability for the first component.
#'
#' @return A numeric vector of quantiles corresponding to \code{p}.
#'
#' @details
#' This function is a vectorized version of \code{\link{qmixpois1}} using \code{mapply()}
#' to compute quantiles efficiently.
#'
#' @examples
#' qmixpois(c(0.1, 0.5, 0.9), lam1 = 2, lam2 = 5, prob = 0.6)
#'
#' @export
qmixpois = function(p, lam1, lam2, prob) {
  # Ensure p, lam1, and lam2 are vectors
  p = as.vector(p)

  # Replicate lam1 and lam2 if they are constants
  if (length(lam1) == 1) {
    lam1 = rep(lam1, length(p))
  }
  if (length(lam2) == 1) {
    lam2 = rep(lam2, length(p))
  }

  # Function to calculate quantile for a single set of parameters
  find_quantile = function(pi, lambda1, lambda2, prob) {
    x = 0
    while (pmixpois1(x, lambda1, lambda2, prob) <= pi) {
      x = x + 1
    }
    return(x)
  }

  # Use mapply to apply the function over vectors p, lam1, and lam2
  quantiles = mapply(find_quantile, p, lam1, lam2, MoreArgs = list(prob = prob))

  return(quantiles)
}

#' Cumulative Distribution Function of the Mixed Poisson Distribution
#'
#' Computes the cumulative distribution function (CDF) of a two-component mixed Poisson distribution.
#'
#' @param x Integer or numeric vector. The value(s) at which to evaluate the CDF.
#' @param lam1 Numeric. Mean of the first Poisson component.
#' @param lam2 Numeric. Mean of the second Poisson component.
#' @param p Numeric between 0 and 1. Mixing probability for the first component.
#'
#' @return A numeric vector of cumulative probabilities corresponding to \code{x}.
#'
#' @details
#' The mixed Poisson CDF is given by:
#' \deqn{p * ppois(x, lam1) + (1 - p) * ppois(x, lam2)}
#'
#' @examples
#' pmixpois1(0:10, lam1 = 2, lam2 = 5, p = 0.6)
#'
#' @export
pmixpois1 = function(x, lam1, lam2,p){
  y = p*ppois(x,lam1) + (1-p)*ppois(x,lam2)
  return(y)
}


#' Density of the Mixed Poisson Distribution
#'
#' Computes the probability mass function (PMF) of a two-component mixed Poisson distribution.
#'
#' @param x Integer or numeric vector. The value(s) at which to evaluate the PMF.
#' @param lam1 Numeric. Mean of the first Poisson component.
#' @param lam2 Numeric. Mean of the second Poisson component.
#' @param p Numeric between 0 and 1. Mixing probability for the first component.
#'
#' @return A numeric vector of probabilities corresponding to \code{x}.
#'
#' @details
#' The mixed Poisson PMF is given by:
#' \deqn{p * dpois(x, lam1) + (1 - p) * dpois(x, lam2)}
#'
#' @examples
#' dmixpois1(0:10, lam1 = 2, lam2 = 5, p = 0.6)
#'
#' @export
dmixpois1 = function(x, lam1, lam2, p){
  y = p*dpois(x,lam1) + (1-p)*dpois(x,lam2)
  return(y)
}

#-------------------------------------------------------------------------------------#
# these are used when CountDist = "Generalize Poisson". I have now implemented the
# "Generalized Poisson 2" and in the future the following will not be needed.
# Generalized Poisson pdfs

#' Computes the probability mass function (PMF) of the Generalized Poisson distribution
#' as defined in Famoye (1994), parameterized via the mean.
#'
#' @param y Integer or numeric vector. The value(s) at which to evaluate the PMF.
#' @param a Numeric. Dispersion parameter. Must satisfy \code{a > -1/m}.
#' @param m Numeric. Mean of the distribution.
#'
#' @return Numeric vector of probabilities corresponding to \code{y}.
#'
#' @details
#' The Generalized Poisson PMF is given by:
#' \deqn{
#'   P(Y = y) = \frac{k^y (1 + a y)^{y - 1} e^{-k(1 + a y)}}{y!}, \quad \text{where } k = \frac{m}{1 + a m}
#' }
#' This function uses the parametrization through the mean as in Famoye (1994).
#'
#' @references Famoye, F. (1994). On the generalized Poisson regression model.
#' Journal of Statistical Planning and Inference, 41(1), 133–146.
#'
#' @examples
#' dGpois(0:5, a = 0.1, m = 3)
#'
#' @export
dGpois = function(y,a,m){
  #relation 2.4 in Famoye 1994, parametrization through the mean
  k = m/(1+a*m)
  return( k^y * (1+a*y)^(y-1) * exp(-k*(1+a*y)-lgamma(y+1)))
}

# Generalized Poisson cdf for one m---------#
#' Generalized Poisson CDF (single mean)
#'
#' Computes the cumulative distribution function (CDF) of the Generalized Poisson distribution
#' for a single value of \code{m}.
#'
#' @param x Integer or numeric vector. Values at which to evaluate the CDF.
#' @param a Numeric. Dispersion parameter.
#' @param m Numeric. Mean parameter.
#'
#' @return Numeric vector of cumulative probabilities.
#'
#' @seealso \code{\link{pGpois}}, \code{\link{dGpois}}
#'
#' @export
pgpois1 = function(x,a,m){
  M = max(x,0)
  bb = dGpois(0:M,a,m)
  cc = cumsum(bb)
  g = rep(0,length(x))
  g[x>=0] = cc[x+1]
  return(g)
}

#--------- Generalized Poisson pdf for multiple m---------#
#' Generalized Poisson CDF (vectorized)
#'
#' Computes the cumulative distribution function (CDF) of the Generalized Poisson distribution
#' for one or more values of the mean parameter \code{m}.
#'
#' @param x Integer or numeric vector. Values at which to evaluate the CDF.
#' @param a Numeric. Dispersion parameter.
#' @param m Numeric or vector. Mean parameter(s).
#'
#' @return A numeric vector or matrix of cumulative probabilities.
#'
#' @seealso \code{\link{pgpois1}}, \code{\link{qGpois}}
#'
#' @export
pGpois = function(x,a,m){

  if (length(m)>1){
    r = mapply(pgpois1, x=x, a, m = m)
  }else{
    r = pgpois1(x, a, m)
  }
  return(r)
}

#--------- Generalized Poisson icdf for 1 m---------#
#' Generalized Poisson Quantile Function (single mean)
#'
#' Computes the quantile function (inverse CDF) of the Generalized Poisson distribution
#' for a single value of the mean.
#'
#' @param p Numeric vector of probabilities (between 0 and 1).
#' @param a Numeric. Dispersion parameter.
#' @param m Numeric. Mean parameter.
#'
#' @return A numeric vector of quantiles.
#'
#' @seealso \code{\link{qGpois}}, \code{\link{pGpois}}
#'
#' @export
qgpois1 <- function(p,a,m) {
  check.list <- pGpois(0:100,a,m)
  quantile.vec <- rep(-99,length(p))

  for (i in 1:length(p)) {
    x <- which(check.list>=p[i],arr.ind = T)[1]
    quantile.vec[i] <- x-1
  }
  return(quantile.vec)
}

#--------- Generalized Poisson icdf for many m---------#
#' Generalized Poisson Quantile Function (vectorized)
#'
#' Computes the quantile function (inverse CDF) of the Generalized Poisson distribution
#' for one or more values of the mean parameter.
#'
#' @param p Numeric vector of probabilities (between 0 and 1).
#' @param a Numeric. Dispersion parameter.
#' @param m Numeric or vector. Mean parameter(s).
#'
#' @return A numeric vector or matrix of quantiles.
#'
#' @seealso \code{\link{qgpois1}}, \code{\link{rGpois}}
#'
#' @export
qGpois = function(p,a,m){

  if (length(m)>1){
    r = mapply(qgpois1, p=p, a, m = m)
  }else{
    r = qgpois1(p,a,m)
  }
  return(r)
}

#--------- Generate Gen Poisson datas---------#
#' Random Generation from Generalized Poisson Distribution
#'
#' Generates random observations from the Generalized Poisson distribution via inverse transform sampling.
#'
#' @param n Integer. Number of observations to generate.
#' @param a Numeric. Dispersion parameter.
#' @param m Numeric or vector. Mean parameter(s).
#'
#' @return A numeric vector of random values drawn from the Generalized Poisson distribution.
#'
#' @seealso \code{\link{qGpois}}, \code{\link{dGpois}}
#'
#' @examples
#' rGpois(10, a = 0.1, m = 3)
#'
#' @export
rGpois = function(n, a,m){
  u = runif(n)
  x = qGpois(u,a, m)
  return(x)
}
#-------------------------------------------------------------------------------------#

#' Generate Random Model Parameters for Testing
#'
#' Randomly generates marginal distribution and ARMA parameters for simulation and
#' testing purposes. Includes a mechanism to occasionally generate "bad" parameters
#' (e.g., invalid or edge-case values) with controlled probability, useful for
#' stress-testing estimation and validation routines.
#'
#' @param CountDist Character string. The name of the count distribution. Supported
#'   values include \code{"Poisson"}, \code{"Negative Binomial"}, \code{"Mixed Poisson"}, and \code{"ZIP"}.
#' @param BadParamProb Numeric between 0 and 1. Probability of sampling "good" vs. "bad"
#'   parameter values. A value close to 1 favors well-behaved parameters.
#' @param AROrder Integer. Order of the autoregressive (AR) component. If 0, no AR terms are generated.
#' @param MAOrder Integer. Order of the moving average (MA) component. If 0, no MA terms are generated.
#' @param Regressor Optional matrix or data frame of covariates. If supplied, dynamic
#'   models with covariate-based parameterization are used.
#'
#' @return A named list with three components:
#' \describe{
#'   \item{\code{MargParm}}{Vector of marginal distribution parameters. Structure depends on \code{CountDist} and whether \code{Regressor} is provided.}
#'   \item{\code{ARParm}}{Autoregressive parameter(s), or \code{NULL} if \code{AROrder = 0}.}
#'   \item{\code{MAParm}}{Moving average parameter(s), or \code{NULL} if \code{MAOrder = 0}.}
#' }
#'
#' @details
#' For testing robustness of model fitting procedures, this function sometimes samples
#' marginal or ARMA parameters from values known to be problematic (e.g., negative rates,
#' non-stationary AR roots). This is controlled by the \code{BadParamProb} parameter.
#'
#' When \code{Regressor} is not \code{NULL}, the function returns parameters assuming a log-linear
#' model with regression effects (e.g., intercept and slope coefficients).
#'
#' @export
GenModelParam = function(CountDist,BadParamProb, AROrder, MAOrder, Regressor){

  #-----------------Marginal Parameters

  if(CountDist=="Poisson"){
    # create some "bad" Poisson parameter choices
    BadLambda = c(-1,500)

    # sample with high probability from "good" choices for lambda and with low prob the "bad" choices
    prob = rbinom(1,1,BadParamProb)

    # Marginal Parameter
    if (is.null(Regressor)){
      MargParm = prob*runif(1,0,100) + (1-prob)*sample(BadLambda,1)
    }else{
      # fix me: I am hard coding 1.2 here but we should probably make this an argument
      b0 = runif(1,0,1.2)
      b1 = runif(1,0,1.2)
      MargParm = c(b0,b1)
    }
  }


  if(CountDist=="Negative Binomial"){
    # create some "bad" Poisson parameter choices
    Badr_r = c(-1,500)

    # sample a probability
    p =  runif(1,0,1)

    # sample with high probability from "good" choices for lambda and with low prob the "bad" choices
    prob = rbinom(1,1,BadParamProb)

    # Marginal Parameter
    if (is.null(Regressor)){
      r = prob*runif(1,0,100) + (1-prob)*sample(Badr_r,1)
      MargParm = c(r,p)
    }else{
      # fix me: I am hard coding 1.2 here but we should probably make this an argument
      b0 = runif(1,0,1.2)
      b1 = runif(1,0,1.2)
      k  = runif(1,0,1)
      MargParm = c(b0,b1,k)
    }
  }


  if(CountDist=="Mixed Poisson"){
    # create some "bad" Poisson parameter choices
    BadLambda = c(-1,500)

    # sample a probability
    p =  runif(1,0,1)

    # sample with high probability from "good" choices for lambda and with low prob the "bad" choices
    prob = rbinom(2,1,BadParamProb)

    # Marginal Parameter
    if (is.null(Regressor)){
      lambda1 = prob[1]*runif(1,0,100) + (1-prob[1])*sample(BadLambda,1)
      lambda2 = prob[2]*runif(1,0,100) + (1-prob[2])*sample(BadLambda,1)
      MargParm = c(lambda1, lambda2, p)
    }else{
      # fix me: I am hard coding 1.2 here but we should probably make this an argument
      b0 = runif(1,0,1.2)
      b1 = runif(1,0,1.2)
      c0 = runif(1,0,1.2)
      c1 = runif(1,0,1.2)
      k  = runif(1,0,1)
      MargParm = c(b0,b1,c0,c1,k)
    }
  }


  if(CountDist=="ZIP"){
    # create some "bad" ZIP parameter choices
    BadLambda = c(-1,500)

    # sample a probability
    p =  runif(1,0,1)

    # sample with high probability from "good" choices for lambda and with low prob the "bad" choices
    prob = rbinom(1,1,BadParamProb)

    # Marginal Parameter
    if (is.null(Regressor)){
      lambda = prob*runif(1,0,100) + (1-prob)*sample(BadLambda,1)
      MargParm = c(lambda, p)
    }else{
      # fix me: I am hard coding 1.2 here but we should probably make this an argument
      b0 = runif(1,0,1.2)
      b1 = runif(1,0,1.2)
      c0 = runif(1,0,1.2)
      c1 = runif(1,0,1.2)
      MargParm = c(b0,b1,c0, c1)
    }
  }
  #------------------ARMA Parameters

  # create some "bad" AR parameter choices
  BadAR = c(0, -10, 0.99, 1.2)

  # create some "bad" MA parameter choices
  BadMA = c(0, 10, -0.99, -1.2)

  # set the AR Parameters
  ARParm = NULL
  MAParm = NULL
  p = rbinom(1,1,BadParamProb)

  if(AROrder) ARParm = p*runif(1,-1, 1)+ (1-p)*sample(BadAR,1)
  if(MAOrder) MAParm = p*runif(1,-1, 1)+ (1-p)*sample(BadMA,1)

  AllParms = list(MargParm, ARParm, MAParm)
  names(AllParms) = c("MargParm", "ARParm", "MAParm")
  return(AllParms)

}

#' Generate Perturbed Initial Parameter Values for Testing
#'
#' Applies a small perturbation to a given set of model parameters to generate
#' initial values for optimization or testing purposes.
#'
#' @param AllParms A named list containing model parameters with components:
#'   \itemize{
#'     \item \code{MargParm}: vector of marginal distribution parameters
#'     \item \code{ARParm}: vector of autoregressive (AR) parameters, or \code{NULL}
#'     \item \code{MAParm}: vector of moving average (MA) parameters, or \code{NULL}
#'   }
#' @param perturbation Numeric scalar. Proportion by which to perturb the parameters
#'   (e.g., \code{0.1} applies a 10% change).
#'
#' @return A named list with the same structure as \code{AllParms}, containing
#' perturbed parameter values. If any of the original parameters are \code{NULL},
#' they remain \code{NULL} in the output.
#'
#' @details
#' The perturbation is applied multiplicatively and negatively:
#' \deqn{\theta_{\text{init}} = \theta \cdot (1 - \text{perturbation})}
#' This helps avoid starting optimization from true parameters, which could
#' mask numerical or convergence issues.
#'
#' @examples
#' true_parms <- list(MargParm = c(1, 0.5), ARParm = c(0.3), MAParm = NULL)
#' GenInitVal(true_parms, perturbation = 0.1)
#'
#' @export
GenInitVal = function(AllParms, perturbation){

  # select negative or positive perturbation based on a  coin flip
  #sign = 1-2*rbinom(1,1,0.5)
  # adding only negative sign now to ensure stability
  sign  = -1

  MargParm = AllParms$MargParm*(1+sign*perturbation)

  PerturbedValues = list(MargParm,NULL,NULL)
  names(PerturbedValues) = c("MargParm", "ARParm", "MAParm")

  if(!is.null(AllParms$ARParm))  PerturbedValues$ARParm = AllParms$ARParm*(1+sign*perturbation)
  if(!is.null(AllParms$MAParm))  PerturbedValues$MAParm = AllParms$MAParm*(1+sign*perturbation)

  return(PerturbedValues)
}

#' Compute Residuals for Latent Gaussian Count Model
#'
#' @param lgc A list object containing the outcome of a model fit.
#
#' @return A numeric vector of residuals.
#'
#' @export
ComputeResiduals= function(lgc){
  #--------------------------------------------------------------------------#
  # PURPOSE:  Compute the residuals in relation (41)
  #
  #--------------------------------------------------------------------------#

  # retrieve marginal distribution parameters
  theta      = lgc$ParamEstimates
  mod        = lgc$mod
  Parms      = RetrieveParameters(theta,mod)
  nMargParms = length(Parms$MargParms)
  nparms     = length(theta)

  # check if the number of parameters matches the model setting
  if(nMargParms + sum(mod$ARMAModel)!=nparms) stop('The length of the provided estimated parameters does not
                                                   match the model specification.')

  # allocate memory
  Zhat <- rep(NA,mod$n)

  # pGpois function doesn't allow for vector lambda so a for-loop is needed, however the change shouldn't
  # be too hard. note the formula subtracts 1 from the counts mod$DependentVar so it may lead to negative numbers
  # that's why there is and if else below.
  for (i in 1:mod$n){
    k <- mod$DependentVar[i]
    if (k != 0) {
      # Compute limits
      if(mod$nreg==0){
        C_xt_minus1 = mod$mycdf(k-1,t(Parms$MargParms))
        C_xt        = mod$mycdf(k,t(Parms$MargParms))
        }else{
          C_xt_minus1 = mod$mycdf(k-1, Parms$ConstMargParm, Parms$DynamMargParm[i])
          C_xt        = mod$mycdf(k, Parms$ConstMargParm, Parms$DynamMargParm[i])
      }
      a           = qnorm(C_xt_minus1,0,1)
      b           = qnorm(C_xt ,0,1)
      ResidNum    = exp(-a^2/2)-exp(-b^2/2)
      ResidDenom  = sqrt(2*pi)*(C_xt-C_xt_minus1)
    }else{
      if(mod$nreg==0){
        # Compute integral limits
        C_xt        = mod$mycdf(0,t(Parms$MargParms))
        b           = qnorm(C_xt,0,1)
      }else{
        C_xt        = mod$mycdf(0, Parms$ConstMargParm, Parms$DynamMargParm[i])
        b           = qnorm(C_xt ,0,1)
      }
      ResidNum    = -exp(-b^2/2)
      ResidDenom  = sqrt(2*pi)*C_xt
    }
    Zhat[i]     =  ResidNum/ResidDenom
  }

  # apply AR filter--Fix me allow for MA as well -commenting out the code below to use arima
  # function. I am also adding centering step.
  # residualOld = data.frame(filter(Zhat,c(1,-Parms$AR))[1:(mod$n-mod$ARMAModel[1])])
  # names(residualOld) = "residual"

  # demean the data - initially the centering step was skipped
  DemeanRes =  Zhat - mean(Zhat)

  # Fit ARMA with fixed parameters
  fit = arima(DemeanRes,
               order = c(mod$nAR,0,mod$nMA),
               fixed = c(Parms$AR,Parms$MA, 0), # AR1, AR2, AR3, intercept
               transform.pars = FALSE)

  residual = as.numeric(residuals(fit))
  names(residual) <- NULL

  return(residual)
}


#' Retrieve Name of Current Marginal Distribution with Parameters
#'
#' Constructs a human-readable string that describes the marginal count distribution
#' used in the model, including its name and parameter values formatted to two decimals.
#'
#' @param theta Numeric vector. The full parameter vector, from which the marginal
#'   parameters will be extracted using model structure from \code{mod}.
#' @param mod A list object containing model specifications. Must include:
#'   \itemize{
#'     \item \code{CountDist}: character name of the marginal distribution (e.g., "Poisson")
#'     \item \code{parmnames}: character vector of parameter names
#'     \item \code{nMargParms}: number of marginal parameters
#'   }
#'
#' @return A character string of the form \code{"DistName(param1=..., param2=..., ...)"}.
#'
#' @examples
#' mod <- list(
#'   CountDist = "Poisson",
#'   parmnames = c("lambda"),
#'   nMargParms = 1
#' )
#' theta <- c(2.35)
#' CurrentDist(theta, mod)
#' # Returns: "Poisson(lambda=2.35)"
#'
#' @export
CurrentDist = function(theta,mod){
  # check me: does it work for models with regressors?
  name = sprintf("%s=%.2f",mod$parmnames[1],theta[1])

  if (mod$nMargParms>1){
    for(i in 2:mod$nMargParms){
      #name = paste(name,initialParam[i],sep=",")
      a = sprintf("%s=%.2f",mod$parmnames[i],theta[i])
      name = paste(name,a,sep=",")
    }
  }
  name = paste(mod$CountDist, "(", name,")", sep="")
  return(name)
}

#' Compute Predictive Distribution via Particle Filtering
#'
#' Modifies the standard particle filtering procedure to return the **predictive distribution**
#' at each time point of a latent Gaussian count time series model, instead of just the log-likelihood.
#'
#' @param theta Numeric vector. Parameter vector containing marginal and ARMA parameters.
#' @param mod A list with model specifications, including:
#'   \itemize{
#'     \item \code{DependentVar}: observed count series
#'     \item \code{CountDist}: count distribution (e.g., "Poisson", "Negative Binomial")
#'     \item \code{ARMAModel}: vector of length 2 specifying AR and MA orders
#'     \item \code{ParticleNumber}: number of particles to use
#'     \item \code{nreg}: number of regressors
#'     \item \code{mypdf}: function to compute marginal PDF
#'     \item \code{mycdf}: function to compute marginal CDF
#'     \item \code{n}: sample size
#'     \item \code{maxdiff}: convergence threshold for Innovations Algorithm
#'   }
#'
#' @return A numeric matrix of dimension \code{2 x n}, where each column corresponds to a time point:
#'   \itemize{
#'     \item First row: lower tail probability for the observed count
#'     \item Second row: predictive probability mass at \code{Y_t + 1}
#'   }
#'
#' @details
#' This function runs a particle filter with resampling to estimate the predictive distribution
#' of the latent state at each time point. It leverages the Innovations Algorithm to calculate
#' ARMA-based forecasts of the latent process and approximates conditional distributions using
#' truncated normal draws.
#'
#' The predictive distribution is calculated by integrating over the particle population, adjusted
#' via importance weights. If any weights become numerically unstable (e.g., sum to 0 or include NA),
#' the function returns a large penalty value (\code{1e8}).
#'
#' Random number generator state is preserved using \code{get_rand_state()} and \code{set_rand_state()}.
#'
#' @references
#' Kechagias, S., Livsey, J., & Pipiras, V. (2020). Latent Gaussian Count Time Series Modeling.
#' \emph{arXiv:1811.00203}. \url{https://arxiv.org/abs/1811.00203}
#'
#' @seealso \code{\link{ComputeWeights}}, \code{\link{SampleTruncNormParticles}}, \code{\link{InnovAlg}}
#'
#' @examples
#' \dontrun{
#' mod <- list(DependentVar = y, CountDist = "Poisson", ARMAModel = c(1, 0),
#'             ParticleNumber = 100, mycdf = ppois, mypdf = dpois, n = length(y),
#'             maxdiff = 1e-4, nreg = 0)
#' theta <- c(2.5, 0.3)
#' PDvalues(theta, mod)
#' }
#'
#' @export
PDvalues = function(theta, mod){
  #------------------------------------------------------------------------------------#
  # PURPOSE:  Compute predictive distribution
  #
  #
  # AUTHORS: James Livsey, Vladas Pipiras, Stefanos Kechagias,
  # DATE:    July 2020
  #------------------------------------------------------------------------------------#

  old_state <- get_rand_state()
  on.exit(set_rand_state(old_state))

  # Retrieve parameters and save them in a list called Parms
  Parms = RetrieveParameters(theta,mod)

  # check for causality
  if( CheckStability(Parms$AR,Parms$MA) ) return(10^(8))

  # Initialize the negative log likelihood computation
  nloglik = ifelse(mod$nreg==0,  - log(mod$mypdf(mod$DependentVar[1],Parms$MargParms)),
                   - log(mod$mypdf(mod$DependentVar[1], Parms$ConstMargParm, Parms$DynamMargParm[1])))

  # retrieve AR, MA orders and their max
  m = max(mod$ARMAModel)
  p = mod$ARMAModel[1]
  q = mod$ARMAModel[2]

  # Compute ARMA covariance up to lag n-1
  a        = list()
  if(!is.null(Parms$AR)){
    a$phi = Parms$AR
  }else{
    a$phi = 0
  }
  if(!is.null(Parms$MA)){
    a$theta = Parms$MA
  }else{
    a$theta = 0
  }
  a$sigma2 = 1
  gamma    = itsmr::aacvf(a,mod$n)

  # Compute coefficients of Innovations Algorithm see 5.2.16 and 5.3.9 in in Brockwell Davis book
  IA       = InnovAlg(Parms, gamma, mod)
  Theta    = IA$thetas
  Rt       = sqrt(IA$v)

  # Get the n such that |v_n-v_{n-1}|< mod$maxdiff. check me: does this guarantee convergence of Thetas?
  nTheta   = IA$n
  Theta_n  = Theta[[nTheta]]

  # allocate matrices for weights, particles and predictions of the latent series
  w        = matrix(0, mod$n, mod$ParticleNumber)
  Z        = matrix(0, mod$n, mod$ParticleNumber)
  Zhat     = matrix(0, mod$n, mod$ParticleNumber)

  # to collect the values of predictive distribution of interest
  preddist = matrix(0,2,mod$n)

  # initialize particle filter weights
  w[1,]    = rep(1,mod$ParticleNumber)

  # Compute the first integral limits Limit$a and Limit$b
  Limit    = ComputeLimits(mod, Parms, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))

  # Initialize the particles using N(0,1) variables truncated to the limits computed above
  #Z[1,]    = SampleTruncNormParticles(mod, Limit$a, Limit$b, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
  Z[1,]    = SampleTruncNormParticles(mod, Limit, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))


  for (t in 2:mod$n){

    # compute Zhat_t
    Zhat[t,] = ComputeZhat_t(mod, IA, Z, Zhat,t, Parms)

    # compute a,b and temp
    temp  = rep(0,(mod$DependentVar[t]+1))
    index = min(t, max(mod$ARMAModel))
    for (x in 0:mod$DependentVar[t]){
      # Compute integral limits
      if(mod$nreg==0){
        a = as.numeric(qnorm(mod$mycdf(x-1,Parms$MargParms),0,1) -  Zhat[t,])/Rt[index]
        b = as.numeric(qnorm(mod$mycdf(x,  Parms$MargParms),0,1) -  Zhat[t,])/Rt[index]
      }else{
        a = as.numeric(qnorm(mod$mycdf(x-1,Parms$ConstMargParm, Parms$DynamMargParm[t]),0,1) -  Zhat[t,])/Rt[index]
        b = as.numeric(qnorm(mod$mycdf(x,  Parms$ConstMargParm, Parms$DynamMargParm[t]),0,1) -  Zhat[t,])/Rt[index]
      }
      temp[x+1] = mean(pnorm(b,0,1) - pnorm(a,0,1))
    }

    # Compute integral limits
    Limit = ComputeLimits(mod, Parms, t, Zhat[t,], Rt)

    # Sample truncated normal particles
    Znew  = SampleTruncNormParticles(mod, Limit, t, Zhat[t,], Rt)

    # update weights
    w[t,] = ComputeWeights(mod, Limit, t, w[(t-1),])

    # check me: break if I got NA weight
    if (any(is.na(w[t,]))| sum(w[t,])==0 ){
      message(sprintf('WARNING: At t=%.0f some of the weights are either too small or sum to 0',t))
      return(10^8)
    }
    # Resample the particles using common random numbers
    old_state1 = get_rand_state()
    Znew = ResampleParticles(mod, w, t, Znew)
    set_rand_state(old_state1)

    # save the current particle
    Z[t,]   = Znew


    if (mod$DependentVar[t]==0){
      preddist[,t] = c(0,temp[1])
    }else{
      preddist[,t] = cumsum(temp)[mod$DependentVar[t]:(mod$DependentVar[t]+1)]
    }

  }

  return(preddist)
}

#' Compute Probability Integral Transform (PIT) Histogram Values
#'
#' Computes histogram values for the Probability Integral Transform (PIT) as described
#' in relations (39)–(40) of the JASA paper on latent Gaussian count time series modeling.
#' This function processes predictive distributions and outputs the PIT histogram bins
#' for calibration assessment.
#'
#' @param H Integer. Number of histogram bins to divide the \[0,1\] PIT range.
#' @param predDist A numeric matrix of dimension \code{2 x n}, typically returned by
#'   \code{\link{PDvalues}}. Each column corresponds to a time point, with:
#'   \itemize{
#'     \item First row: lower tail probability of the observed count
#'     \item Second row: cumulative probability including the observed count
#'   }
#'
#' @return A numeric vector of length \code{H} representing the PIT histogram values (bin heights).
#' These can be plotted to assess calibration of predictive distributions.
#'
#' @details
#' The PIT is used to assess the **calibration** of probabilistic forecasts. For discrete data,
#' the PIT is computed using randomized methods or as a piecewise function, as detailed in
#' the latent Gaussian count models literature.
#'
#' This implementation applies the formulas:
#' \deqn{PIT(Y_t) \sim U(0,1)} if predictive distributions are correctly specified.
#'
#' @references
#' Kechagias, S., Livsey, J., & Pipiras, V. (2020).
#' \emph{Latent Gaussian Count Time Series Modeling}, Journal of the American Statistical Association.
#' \url{https://arxiv.org/abs/1811.00203}
#'
#' @seealso \code{\link{PDvalues}}, \code{\link{ComputeResiduals}}
#'
#' @examples
#' \dontrun{
#' predDist <- PDvalues(theta, mod)
#' hist(PITvalues(10, predDist), breaks = 10, main = "PIT Histogram")
#' }
#'
#' @export
PITvalues = function(H, predDist){
  PITvalues = rep(0,H)

  predd1 = predDist[1,]
  predd2 = predDist[2,]
  Tr = length(predd1)

  for (h in 1:H){
    id1 = (predd1 < h/H)*(h/H < predd2)
    id2 = (h/H >= predd2)
    tmp1 = (h/H-predd1)/(predd2-predd1)
    tmp1[!id1] = 0
    tmp2 = rep(0,Tr)
    tmp2[id2] = 1
    PITvalues[h] = mean(tmp1+tmp2)
  }
PITvalues = c(0,PITvalues)
return(diff(PITvalues))
}


#' Parse Formula into Response and Regressors
#'
#' Parses a standard R formula and extracts the response variable, predictor variables,
#' and intercept status.
#'
#' @param formula An object of class \code{formula}, typically in the form \code{y ~ x1 + x2}.
#'
#' @return A list with three elements:
#' \describe{
#'   \item{\code{DependentVar}}{Character string of the response variable name.}
#'   \item{\code{Regressor}}{Character vector of predictor variable names, or \code{NULL} if none.}
#'   \item{\code{intercept}}{Logical, \code{TRUE} if an intercept is included, \code{FALSE} otherwise.}
#' }
#'
#' @details
#' This function is useful for preprocessing model formulas in custom modeling functions.
#' It uses \code{\link{terms}} to extract structure from the formula object.
#'
#'
#' @seealso \code{\link{terms}}, \code{\link{model.frame}}
#'
#' @export
parse_formula <- function(formula) {
  if (!inherits(formula, "formula")) {
    stop("Input must be a formula.")
  }

  # Extract the terms object from the formula
  terms_obj <- terms(formula)

  # Extract the response variable (left-hand side)
  DependentVar <- as.character(formula[[2]])

  # Extract the predictor terms (right-hand side)
  Regressor <- attr(terms_obj, "term.labels")

  # If there are no predictors, return NULL
  if (length(Regressor) == 0) {
    Regressor <- NULL
  }

  # Check if the intercept is included (1 if included, 0 if excluded)
  has_intercept <- attr(terms_obj, "intercept") == 1

  # Return a list with response, predictors, and intercept status
  return(list(
    DependentVar = DependentVar,
    Regressor = Regressor,
    intercept = has_intercept
  ))
}


#===========================================================================================#
#--------------------------------------------------------------------------------#
# On August 2024 for Issue #27, I created a modified likelihood function called
# ParticleFilter_Res_ARMA_MisSpec with three changes:
#
# 1. The value 1 for the count CDF calculations is changed to 1-10^(-16), so that
#    when inverse normal is applied in the copula, we do not receive an inf value.
# 2. When the limits of the truncated normal distribution become too large (say>7),
#    I set them equal to 7-epsilon, so when I apply the normal cdf and subsequnetly the
#    inverse cdf i avoid the inf values.
# 3. If the weights in the particle filter likelihood become 0 I set them equal to
#    10^(-64).
#
# I have added the changes in the ParticleFilter_Res_ARMA_MisSpec, ComputeLimits_MisSpec
# and SampleTruncNormParticles_MisSpec files and will continue  working with the
# standard versions below until I perform adequate testing.


# PF likelihood with resampling for ARMA(p,q) - with adhoc truncations
ParticleFilter_Res_ARMA_MisSpec = function(theta, mod){
  #------------------------------------------------------------------------------------#
  # PURPOSE:  Use particle filtering with resampling to approximate the likelihood
  #           of the a specified count time series model with an underlying MA(1)
  #           dependence structure.
  #
  # NOTES:    1. See "Latent Gaussian Count Time Series Modeling" for  more
  #           details. A first version of the paper can be found at:
  #           https://arxiv.org/abs/1811.00203
  #           2. This function is very similar to LikSISGenDist_ARp but here
  #           I have a resampling step.
  #
  # INPUTS:
  #    theta:            parameter vector
  #    data:             data
  #    ParticleNumber:   number of particles to be used.
  #    Regressor:        independent variable
  #    CountDist:        count marginal distribution
  #    epsilon           resampling when ESS<epsilon*N
  #
  # OUTPUT:
  #    loglik:           approximate log-likelihood
  #
  #
  # AUTHORS: James Livsey, Vladas Pipiras, Stefanos Kechagias,
  # DATE:    July 2020
  #------------------------------------------------------------------------------------#

  old_state <- get_rand_state()
  on.exit(set_rand_state(old_state))

  # Retrieve parameters and save them in a list called Parms
  Parms = RetrieveParameters(theta,mod)

  # check for causality
  if( CheckStability(Parms$AR,Parms$MA) ) return(10^(8))

  # Initialize the negative log likelihood computation
  nloglik = ifelse(mod$nreg==0,  - log(mod$mypdf(mod$DependentVar[1],Parms$MargParms)),
                   - log(mod$mypdf(mod$DependentVar[1], Parms$ConstMargParm, Parms$DynamMargParm[1])))

  # retrieve AR, MA orders and their max
  m = max(mod$ARMAModel)
  p = mod$ARMAModel[1]
  q = mod$ARMAModel[2]


  # Compute ARMA covariance up to lag n-1
  a        = list()
  if(!is.null(Parms$AR)){
    a$phi = Parms$AR
  }else{
    a$phi = 0
  }
  if(!is.null(Parms$MA)){
    a$theta = Parms$MA
  }else{
    a$theta = 0
  }
  a$sigma2 = 1
  gamma    = itsmr::aacvf(a,mod$n)

  # Compute coefficients of Innovations Algorithm see 5.2.16 and 5.3.9 in in Brockwell Davis book
  IA       = InnovAlg(Parms, gamma, mod)
  Theta    = IA$thetas
  Rt       = sqrt(IA$v)

  # Get the n such that |v_n-v_{n-1}|< mod$maxdiff. check me: does this guarantee convergence of Thetas?
  nTheta   = IA$n
  Theta_n  = Theta[[nTheta]]

  # allocate matrices for weights, particles and predictions of the latent series
  w        = matrix(0, mod$n, mod$ParticleNumber)
  Z        = matrix(0, mod$n, mod$ParticleNumber)
  Zhat     = matrix(0, mod$n, mod$ParticleNumber)

  # initialize particle filter weights
  w[1,]    = rep(1,mod$ParticleNumber)

  # Compute the first integral limits Limit$a and Limit$b
  Limit    = ComputeLimits_MisSpec(mod, Parms, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))

  # Initialize the particles using N(0,1) variables truncated to the limits computed above
  #Z[1,]    = SampleTruncNormParticles(mod, Limit$a, Limit$b, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
  Z[1,]    = SampleTruncNormParticles_MisSpec(mod, Limit, Parms, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))


  for (t in 2:mod$n){

    # compute Zhat_t
    #Zhat[t,] = ComputeZhat_t(m,Theta,Z,Zhat,t, Parms,p,q, nTheta, Theta_n)
    Zhat[t,] = ComputeZhat_t(mod, IA, Z, Zhat,t, Parms)

    # Compute integral limits
    Limit = ComputeLimits_MisSpec(mod, Parms, t, Zhat[t,], Rt)

    # Sample truncated normal particles
    #Znew  = SampleTruncNormParticles(mod, Limit$a, Limit$b, t, Zhat[t,], Rt)
    Znew  = SampleTruncNormParticles_MisSpec(mod, Limit, Parms, t, Zhat[t,], Rt)

    # update weights
    #w[t,] = ComputeWeights(mod, Limit$a, Limit$b, t, w[(t-1),])
    w[t,] = ComputeWeights(mod, Limit, t, w[(t-1),])
    #print(t)
    #print(w[t,])
    # check me: In misspecified models, the weights may get equal to 0. Is it ok
    # for me to do the following? how is this different from allowing zero weights and
    # returning a large likelihood?
    if (sum(w[t,])==0){
      w[t,] = rep(10^(-64),mod$ParticleNumber)
      message(sprintf("At t = %.0f all weights are equal to 0. They are resetted to equal 1e-64.",t))
    }

    # check me: break if I got NA weight
    if (any(is.na(w[t,]))| sum(w[t,])==0 ){
      #print(t)
      #print(w[t,])
      message(sprintf('WARNING: At t=%.0f some of the weights are either too small or sum to 0',t))
      return(10^8)
    }

    # Resample the particles using common random numbers
    old_state1 = get_rand_state()
    Znew = ResampleParticles(mod, w, t, Znew)
    set_rand_state(old_state1)

    # save the current particle
    Z[t,]   = Znew

    # update likelihood
    nloglik = nloglik - log(mean(w[t,]))

  }

  # for log-likelihood we use a bias correction--see par2.3 in Durbin Koopman, 1997
  # nloglik = nloglik- (1/(2*N))*(var(na.omit(wgh[T1,]))/mean(na.omit(wgh[T1,])))/mean(na.omit(wgh[T1,]))

  # if (nloglik==Inf | is.na(nloglik)){
  #   nloglik = 10^8
  # }


  return(nloglik)
}

ComputeLimits_MisSpec = function(mod, Parms, t, Zhat, Rt){
  # a and b are the arguments in the two normal cdfs in the 4th line in equation (19) in JASA paper
  Lim = list()
  # fix me: this will be ok for AR or MA models but for ARMA? is it p+q instead of max(p,q)
  # fix me: why do I have t(Parms$MargParms)?
  index = min(t, max(mod$ARMAModel))

  # add the following for White Noise models
  if(max(mod$ARMAModel)==0) index=1

  if(mod$nreg==0){

    C_1 = mod$mycdf(mod$DependentVar[t]-1,t(Parms$MargParms))
    C   = mod$mycdf(mod$DependentVar[t],t(Parms$MargParms))

    # fix me: need to implement for regressors as well
    if(C_1==1) {
      C_1 = 1-10^(-16)

      # retrieve the name of the current distribution
      CurrentDist = CurrentDist(Parms$MargParms, mod)

      # warn the user of the situation
      message(sprintf("To avoid an Inf inverse cdf value, 1e-16 was
subtracted from C_xt_1 in relation (19). C_xt_1 = 1 can be caused by
a misspecified marginal distribution, lack of relevant covariates, or
an optimizer seaching the parameter space away from the truth. Here a
%s model is fitted, but at t=%.0f the dependent variable is
equal to %.0f.\n",CurrentDist, t,mod$DependentVar[t]))
    }
    if(C==1 ) {
      # retrieve the name of the current distribution
      CurrentDist = CurrentDist(Parms$MargParms, mod)

      C = 1-10^(-16)
      message(sprintf("To avoid an Inf inverse cdf value, 1e-16 was
subtracted from C_xt in relation (19). C_xt_1 = 1 can be caused by
a misspecified marginal distribution, lack of relevant covariates,
or an optimizer seaching the parameter space away from the truth. Here
a %s model is fitted, but at t=%.0f the dependent variable is
equal to %.0f.\n",CurrentDist,t,mod$DependentVar[t]))
    }

    Lim$a = as.numeric((qnorm(C_1,0,1)) - Zhat)/Rt[index]
    Lim$b = as.numeric((qnorm(C  ,0,1)) - Zhat)/Rt[index]
  }else{
    Lim$a = as.numeric((qnorm(mod$mycdf(mod$DependentVar[t]-1,Parms$ConstMargParm, Parms$DynamMargParm[t,]),0,1)) - Zhat)/Rt[index]
    Lim$b = as.numeric((qnorm(mod$mycdf(mod$DependentVar[t],Parms$ConstMargParm, Parms$DynamMargParm[t,]),0,1)) - Zhat)/Rt[index]
  }

  return(Lim)
}

SampleTruncNormParticles_MisSpec = function(mod, Limit, Parms, t, Zhat, Rt){
  # relation (21) in JASA paper and the inverse transform method
  # check me: this can be improved?
  index = min(t, max(mod$ARMAModel))
  if(max(mod$ARMAModel)==0){index=1}


  # Check me: in the case of missspecifed models, I may get really large values for the limits
  # wchich means that the pnorm will equal 1 and then the qnorm will yield Inf. Is this ok?
  if (min(Limit$a)>=7 ) {
    # retrieve the name of the current distribution
    CurrentDist = CurrentDist(Parms$MargParms, mod)
    Limit$a[Limit$a>=7] = 7 - 10^(-11)
    message(sprintf("To avoid an Inf inverse cdf value, 1e-11 was subtracted from
the lower limit of the truncated Normal distribution used to generate new particles.
Large limits can be caused by a misspecified marginal distribution, lack of relevant
covariates, or an optimizer seaching the parameter space away from the truth among
other things. Here a %s model is fitted, but at t=%.0f the dependent
variable is equal to %.0f.\n", CurrentDist, t,mod$DependentVar[t]))
  }

  #   if (max(Limit$a)<=-7 ) {
  #     # retrieve the name of the current distribution
  #     CurrentDist = CurrentDist(Parms$MargParms, mod)
  #     Limit$a[Limit$a<=-7] = -7 + 10^(-11)
  #     message(sprintf("To avoid an Inf inverse cdf value, 1e-11 was added to
  # the lower limit of the truncated Normal distribution used to generate new particles.
  # Large limits can be caused by a misspecified marginal distribution, lack of relevant
  # covariates, or an optimizer seaching the parameter space away from the truth among
  # other things. Here a %s model is fitted, but at t=%.0f the dependent
  # variable is equal to %.0f.\n", CurrentDist,t,mod$DependentVar[t]))
  #     }

  if (min(Limit$b)>=7 ) {
    # retrieve the name of the current distribution
    CurrentDist = CurrentDist(Parms$MargParms, mod)
    Limit$b[Limit$b>=7] = 7 - 10^(-11)
    message(sprintf("To avoid an Inf inverse cdf value, 1e-11 was subtracted from
the upper limit of the truncated Normal distribution used to generate new particles.
Large limits can be caused by a misspecified marginal distribution, lack of relevant
covariates, or an optimizer seaching the parameter space away from the truth among
other things. Here a %s model is fitted, but at t=%.0f the dependent
variable is equal to %.0f.\n", CurrentDist, t,mod$DependentVar[t]))
  }

  #   if (max(Limit$b)<=-7 ) {
  #     # retrieve the name of the current distribution
  #     CurrentDist = CurrentDist(Parms$MargParms, mod)
  #     Limit$b[Limit$b<=-7] = -7 + 10^(-11)
  #     message(sprintf("To avoid an Inf inverse cdf value, 1e-11 was added to
  # the upper limit of the truncated Normal distribution used to generate new particles.
  # Large limits can be caused by a misspecified marginal distribution, lack of relevant
  # covariates, or an optimizer seaching the parameter space away from the truth among
  # other things. Here a %s model is fitted, but at t=%.0f the dependent
  # variable is equal to %.0f.", CurrentDist,t,mod$DependentVar[t]))
  #   }
  z = qnorm(runif(length(Limit$a),0,1)*(pnorm(Limit$b,0,1)-pnorm(Limit$a,0,1))+pnorm(Limit$a,0,1),0,1)*Rt[index] + Zhat

  return(z)
}

#############################################################################################
#-------------------------------------------------------------------------------------------#
# functions we wrote, and may use in the future
# poisson cdf using incomplete gamma and its derivative wrt to lambda

# myppois = function(x, lambda){
#   # compute poisson cdf as the ratio of an incomplete gamma function over the standard gamma function
#   # I will also compute the derivative of the poisson cdf wrt lambda
#   X  = c(lambda,x+1)
#   v1 = gammainc(X)
#   v2 = gamma(x+1)
#
#   # straight forward formula from the definition of incomplete gamma integral
#   v1_d = -lambda^x*exp(-lambda)
#   v2_d = 0
#
#   z  = v1/v2
#   z_d = (v1_d*v2 - v2_d*v1)/v2^2
#   return(c(z,z_d))
# }


# PF likelihood with resampling for AR(p)
# ParticleFilter_Res_AR_old = function(theta, mod){
#   #--------------------------------------------------------------------------#
#   # PURPOSE:  Use particle filtering with resampling
#   #           to approximate the likelihood of the
#   #           a specified count time series model with an underlying AR(p)
#   #           dependence structure.
#   #
#   #
#   # INPUTS:
#   #    theta: parameter vector
#   #      mod: a list containing all t he information for the model, such as
#   #           count distribution. ARMA model, etc
#   # OUTPUT:
#   #    loglik: approximate log-likelihood
#   #
#   # AUTHORS: James Livsey, Vladas Pipiras, Stefanos Kechagias
#   # DATE:    July  2020
#   #--------------------------------------------------------------------------#
#
#   # keep track of the random seed to use common random numbers
#   old_state <- get_rand_state()
#   on.exit(set_rand_state(old_state))
#
#   # Retrieve parameters ans save them in a li
#   Parms = RetrieveParameters(theta,mod)
#
#   # check for causality
#   if( CheckStability(Parms$AR,Parms$MA) ) return(10^(8))
#
#   # Initialize the negative log likelihood computation
#   nloglik = ifelse(mod$nreg==0,  - log(mod$mypdf(mod$DependentVar[1],Parms$MargParms)),
#                    - log(mod$mypdf(mod$DependentVar[1], Parms$ConstMargParm, Parms$DynamMargParm[1])))
#
#   # Compute the theoretical covariance for the AR model for current estimate
#   gt    = ARMAacf(ar = Parms$AR, ma = Parms$MA,lag.max = mod$n)
#
#   # Compute the best linear predictor coefficients and errors using Durbin Levinson
#   Phi = list()
#   for (t in 2:mod$n){
#     CurrentDL     = DLAcfToAR(gt[2:t])
#     Phi[[t-1]] = CurrentDL[,1]
#   }
#   Rt           = c(1,sqrt(as.numeric(CurrentDL[,3])))
#
#   # allocate memory for particle weights and the latent Gaussian Series particles, check me: do I weights for 1:T or only 2?
#   w     = matrix(0, mod$n, mod$ParticleNumber)
#   Z     = matrix(0, max(mod$ARMAModel), mod$ParticleNumber)
#
#   #======================   Start the SIS algorithm   ======================#
#   # Initialize the weights
#   w[1,] = rep(1,mod$ParticleNumber)
#
#   # Compute the first integral limits Limit$ a and Limit$b
#   Limit = ComputeLimits(mod, Parms, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#
#   # Initialize particles from truncated normal distribution
#   #Z[1,] = SampleTruncNormParticles(mod, Limit$a, Limit$b, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#   Z[1,] = SampleTruncNormParticles(mod, Limit, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#
#
#   # =================== Loop over t ===================== #
#   for (t in 2:mod$n){
#     # Compute the latent Gaussian predictions Zhat_t using Innovations Algorithm
#
#     if(t==2 || mod$ParticleNumber==1){
#       Zhat  =         Z[1:(t-1),] %*% Phi[[t-1]]
#     }else{
#       Zhat  = colSums(Z[1:(t-1),] %*% Phi[[t-1]])
#     }
#
#     # Compute integral limits
#     Limit = ComputeLimits(mod, Parms, t, Zhat, Rt)
#
#     # Sample truncated normal particles
#     #Znew  = SampleTruncNormParticles(mod, Limit$a, Limit$b,t, Zhat, Rt)
#     Znew  = SampleTruncNormParticles(mod, Limit, t, Zhat, Rt)
#
#     # update weights
#     #w[t,] = ComputeWeights(mod, Limit$a, Limit$b, t, w[(t-1),])
#     w[t,] = ComputeWeights(mod, Limit, t, w[(t-1),])
#
#     # check me: break if I got NA weight
#     if (any(is.na(w[t,]))| sum(w[t,])==0 ){
#       message(sprintf('WARNING: Some of the weights are either too small or sum to 0'))
#       return(10^8)
#     }
#
#     # Resample the particles using common random numbers
#     old_state1 = get_rand_state()
#     Znew = ResampleParticles(mod, w, t, Znew)
#     set_rand_state(old_state1)
#
#     # Combine current particles, with particles from previous iterations
#     Z = rbind(Znew, as.matrix(Z[1:(t-1),]))
#     # if (mod$ARMAModel[1]>1){
#     #   Z = rbind(Znew, Z[1:( min(t,max(mod$ARMAModel)) -1),])
#     # }else {
#     #   Z[1,]=Znew
#     # }
#
#     # update log-likelihood
#     nloglik = nloglik - log(mean(w[t,]))
#   }
#
#   return(nloglik)
# }
#
# # PF likelihood with resampling for MA(q)
# ParticleFilter_Res_MA_old = function(theta, mod){
#   #------------------------------------------------------------------------------------#
#   # PURPOSE:  Use particle filtering with resampling to approximate the likelihood
#   #           of the a specified count time series model with an underlying MA(1)
#   #           dependence structure.
#   #
#   # NOTES:    1. See "Latent Gaussian Count Time Series Modeling" for  more
#   #           details. A first version of the paper can be found at:
#   #           https://arxiv.org/abs/1811.00203
#   #           2. This function is very similar to LikSISGenDist_ARp but here
#   #           I have a resampling step.
#   #
#   # INPUTS:
#   #    theta:            parameter vector
#   #    data:             data
#   #    ParticleNumber:   number of particles to be used.
#   #    Regressor:        independent variable
#   #    CountDist:        count marginal distribution
#   #    epsilon           resampling when ESS<epsilon*N
#   #
#   # OUTPUT:
#   #    loglik:           approximate log-likelihood
#   #
#   #
#   # AUTHORS: James Livsey, Vladas Pipiras, Stefanos Kechagias,
#   # DATE:    July 2020
#   #------------------------------------------------------------------------------------#
#
#   old_state <- get_rand_state()
#   on.exit(set_rand_state(old_state))
#
#   # Retrieve parameters and save them in a list called Parms
#   Parms = RetrieveParameters(theta,mod)
#
#   # check for causality
#   if( CheckStability(Parms$AR,Parms$MA) ) return(10^(8))
#
#   # Initialize the negative log likelihood computation
#   nloglik = ifelse(mod$nreg==0,  - log(mod$mypdf(mod$DependentVar[1],Parms$MargParms)),
#                    - log(mod$mypdf(mod$DependentVar[1], Parms$ConstMargParm, Parms$DynamMargParm[1])))
#
#   # Compute covariance up to lag n-1
#   gt    = as.vector(ARMAacf(ar = Parms$AR, ma = Parms$MA, lag.max = mod$n))
#
#   # Compute coefficients of Innovations Algorithm see 5.2.16 and 5.3.9 in in Brockwell Davis book
#   IA    = innovations.algorithm(gt)
#   Theta = IA$thetas
#   Rt    = sqrt(IA$v)
#
#   # allocate matrices for weights, particles and innovations which are equal to Z-Zhat
#   w     = matrix(0, mod$n, mod$ParticleNumber)
#   Z     = matrix(0, max(mod$ARMAModel), mod$ParticleNumber)
#   Inn   = matrix(0, max(mod$ARMAModel), mod$ParticleNumber)
#
#   # particle filter weights
#   w[1,]   = rep(1,mod$ParticleNumber)
#
#   # Compute the first integral limits Limit$ a and Limit$b
#   Limit = ComputeLimits(mod, Parms, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#
#   # Generate N(0,1) variables restricted to (ai,bi),i=1,...n
#   #Z[1,]   = SampleTruncNormParticles(mod, Limit$a, Limit$b, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#   Z[1,]   = SampleTruncNormParticles(mod, Limit, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#
#
#   # Compute the first innovation (Zhat_1=0)
#   Inn[1,] = Z[1,]
#
#
#   for (t in 2:mod$n){
#
#     # Compute the latent Gaussian predictions Zhat_t using Innovations Algorithm - see 5.3.9 in Brockwell Davis book
#     if(t==2 || mod$ParticleNumber==1){
#       Zhat  =         Inn[1:(min(t-1,mod$nMA)),] %*% Theta[[t-1]][1:(min(t-1,mod$nMA))]
#     }else{
#       Zhat  = colSums(Inn[1:(min(t-1,mod$nMA)),] %*% Theta[[t-1]][1:(min(t-1,mod$nMA))])
#     }
#
#     # Compute integral limits
#     Limit = ComputeLimits(mod, Parms, t, Zhat, Rt)
#
#
#     # Sample truncated normal particles
#     #Znew  = SampleTruncNormParticles(mod, Limit$a, Limit$b, t, Zhat, Rt)
#     Znew  = SampleTruncNormParticles(mod, Limit, t, Zhat, Rt)
#
#     # update weights
#     #w[t,] = ComputeWeights(mod, Limit$a, Limit$b, t, w[(t-1),])
#     w[t,] = ComputeWeights(mod, Limit, t, w[(t-1),])
#
#
#     # check me: break if I got NA weight
#     if (any(is.na(w[t,]))| sum(w[t,])==0 ){
#       message(sprintf('WARNING: Some of the weights are either too small or sum to 0'))
#       return(10^8)
#     }
#
#     # Resample the particles using common random numbers
#     old_state1 = get_rand_state()
#     Znew = ResampleParticles(mod, w, t, Znew)
#     set_rand_state(old_state1)
#
#     # Compute the new Innovation
#     InnNew = Znew - Zhat
#
#     # Combine current particles, with particles from previous iterations
#     Inn = rbind(InnNew, as.matrix(Inn[1:min(t-1,mod$nMA),]))
#
#     # update likelihood
#     nloglik = nloglik - log(mean(w[t,]))
#
#   }
#
#   # for log-likelihood we use a bias correction--see par2.3 in Durbin Koopman, 1997
#   # nloglik = nloglik- (1/(2*N))*(var(na.omit(wgh[T1,]))/mean(na.omit(wgh[T1,])))/mean(na.omit(wgh[T1,]))
#
#   # if (nloglik==Inf | is.na(nloglik)){
#   #   nloglik = 10^8
#   # }
#
#
#   return(nloglik)
# }

# PF likelihood with resampling
# ParticleFilter_Res = function(theta, mod){
#   #--------------------------------------------------------------------------#
#   # PURPOSE:  Use particle filtering with resampling
#   #           to approximate the likelihood of the
#   #           a specified count time series model with an underlying AR(p)
#   #           dependence structure or MA(q) structure.
#   #
#   # NOTES:    1. See "Latent Gaussian Count Time Series Modeling" for  more
#   #           details. A first version of the paer can be found at:
#   #           https://arxiv.org/abs/1811.00203
#
#   #
#   # INPUTS:
#   #    theta:            parameter vector
#   #    data:             dependent variable
#   #    Regressor:        independent variables
#   #    ParticleNumber:   number of particles to be used in likelihood approximation
#   #    CountDist:        count marginal distribution
#   #    epsilon           resampling when ESS<epsilon*N
#   #
#   # OUTPUT:
#   #    loglik:           approximate log-likelihood
#   #
#   #
#   # AUTHORS: James Livsey, Vladas Pipiras, Stefanos Kechagias,
#   # DATE:    July 2020
#   #--------------------------------------------------------------------------#
#
#
#   # Pure AR model
#   if(mod$ARMAModel[1]>0 && mod$ARMAModel[2]==0) loglik = ParticleFilter_Res_AR(theta, mod)
#   # Pure MA model or White noise
#   if(mod$ARMAModel[1]==0&& mod$ARMAModel[2]>=0) loglik = ParticleFilter_Res_MA(theta, mod)
#   return(loglik)
# }
#
# # innovations algorithm code - use in previous likelihood implementations
# innovations.algorithm <- function(gamma){
#   n.max=length(gamma)-1
#   # Found this online need to check it
#   # http://faculty.washington.edu/dbp/s519/R-code/innovations-algorithm.R
#   thetas <- vector(mode="list",length=n.max)
#   v <- rep(gamma[1],n.max+1)
#   for(n in 1:n.max){
#     thetas[[n]] <- rep(0,n)
#     thetas[[n]][n] <- gamma[n+1]/v[1]
#     if(n>1){
#       for(k in 1:(n-1)){
#         js <- 0:(k-1)
#         thetas[[n]][n-k] <- (gamma[n-k+1] - sum(thetas[[k]][k-js]*thetas[[n]][n-js]*v[js+1]))/v[k+1]
#       }
#     }
#     js <- 0:(n-1)
#     v[n+1] <- v[n+1] - sum(thetas[[n]][n-js]^2*v[js+1])
#   }
#   v = v/v[1]
#   return(structure(list(v=v,thetas=thetas)))
# }
#
# # PF likelihood with resampling for AR(p) - written more concicely
# ParticleFilter_Res_AR = function(theta, mod){
#   #--------------------------------------------------------------------------#
#   # PURPOSE:  Use particle filtering with resampling
#   #           to approximate the likelihood of the
#   #           a specified count time series model with an underlying AR(p)
#   #           dependence structure.
#   #
#   #
#   # INPUTS:
#   #    theta: parameter vector
#   #      mod: a list containing all t he information for the model, such as
#   #           count distribution. ARMA model, etc
#   # OUTPUT:
#   #    loglik: approximate log-likelihood
#   #
#   # AUTHORS: James Livsey, Vladas Pipiras, Stefanos Kechagias
#   # DATE:    July  2020
#   #--------------------------------------------------------------------------#
#
#   # keep track of the random seed to use common random numbers
#   old_state <- get_rand_state()
#   on.exit(set_rand_state(old_state))
#
#   # Retrieve parameters ans save them in a li
#   Parms = RetrieveParameters(theta,mod)
#
#   # check for causality
#   if( CheckStability(Parms$AR,Parms$MA) ) return(10^(8))
#
#   # Initialize the negative log likelihood computation
#   nloglik = ifelse(mod$nreg==0,  - log(mod$mypdf(mod$DependentVar[1],Parms$MargParms)),
#                    - log(mod$mypdf(mod$DependentVar[1], Parms$ConstMargParm, Parms$DynamMargParm[1])))
#
#   # Compute the theoretical covariance for the AR model for current estimate
#   gt    = ARMAacf(ar = Parms$AR, ma = Parms$MA,lag.max = mod$n)
#
#   # Compute the best linear predictor coefficients and errors using Durbin Levinson
#   Phi = list()
#   for (t in 2:mod$n){
#     CurrentDL     = DLAcfToAR(gt[2:t])
#     Phi[[t-1]] = CurrentDL[,1]
#   }
#   Rt           = c(1,sqrt(as.numeric(CurrentDL[,3])))
#
#   # allocate memory for particle weights and the latent Gaussian Series particles, check me: do I weights for 1:T or only 2?
#   w     = matrix(0, mod$n, mod$ParticleNumber)
#   Z     = matrix(0, max(mod$ARMAModel), mod$ParticleNumber)
#
#   #======================   Start the SIS algorithm   ======================#
#   # Initialize the weights
#   w[1,] = rep(1,mod$ParticleNumber)
#
#   # Compute the first integral limits Limit$ a and Limit$b
#   Limit = ComputeLimits(mod, Parms, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#
#   # Initialize particles from truncated normal distribution
#   #Z[1,] = SampleTruncNormParticles(mod, Limit$a, Limit$b, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#   Z[1,] = SampleTruncNormParticles(mod, Limit, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#
#   # =================== Loop over t ===================== #
#   for (t in 2:mod$n){
#     # Compute the latent Gaussian predictions Zhat_t using Innovations Algorithm
#     Zhat  =         Phi[[t-1]]%*%Z[1:(t-1),]
#
#     # Compute integral limits
#     Limit = ComputeLimits(mod, Parms, t, Zhat, Rt)
#
#     # Sample truncated normal particles
#     #Znew  = SampleTruncNormParticles(mod, Limit$a, Limit$b,t, Zhat, Rt)
#     Znew  = SampleTruncNormParticles(mod, Limit, t, Zhat, Rt)
#
#     # update weights
#     #w[t,] = ComputeWeights(mod, Limit$a, Limit$b, t, w[(t-1),])
#     w[t,] = ComputeWeights(mod, Limit, t, w[(t-1),])
#
#     # check me: break if I got NA weight
#     if (any(is.na(w[t,]))| sum(w[t,])==0 ){
#       message(sprintf('WARNING: Some of the weights are either too small or sum to 0'))
#       return(10^8)
#     }
#
#     # Resample the particles using common random numbers
#     old_state1 = get_rand_state()
#     Znew = ResampleParticles(mod, w, t, Znew)
#     set_rand_state(old_state1)
#
#     # Combine current particles, with particles from previous iterations
#     Z = rbind(Znew, matrix(Z[1:(t-1),],ncol = mod$ParticleNumber))
#
#     # update log-likelihood
#     nloglik = nloglik - log(mean(w[t,]))
#   }
#
#   return(nloglik)
# }
#
# # PF likelihood with resampling for MA(q)
# ParticleFilter_Res_MA = function(theta, mod){
#   #------------------------------------------------------------------------------------#
#   # PURPOSE:  Use particle filtering with resampling to approximate the likelihood
#   #           of the a specified count time series model with an underlying MA(1)
#   #           dependence structure.
#   #
#   # NOTES:    1. See "Latent Gaussian Count Time Series Modeling" for  more
#   #           details. A first version of the paper can be found at:
#   #           https://arxiv.org/abs/1811.00203
#   #           2. This function is very similar to LikSISGenDist_ARp but here
#   #           I have a resampling step.
#   #
#   # INPUTS:
#   #    theta:            parameter vector
#   #    data:             data
#   #    ParticleNumber:   number of particles to be used.
#   #    Regressor:        independent variable
#   #    CountDist:        count marginal distribution
#   #    epsilon           resampling when ESS<epsilon*N
#   #
#   # OUTPUT:
#   #    loglik:           approximate log-likelihood
#   #
#   #
#   # AUTHORS: James Livsey, Vladas Pipiras, Stefanos Kechagias,
#   # DATE:    July 2020
#   #------------------------------------------------------------------------------------#
#
#   old_state <- get_rand_state()
#   on.exit(set_rand_state(old_state))
#
#   # Retrieve parameters and save them in a list called Parms
#   Parms = RetrieveParameters(theta,mod)
#
#   # check for causality
#   if( CheckStability(Parms$AR,Parms$MA) ) return(10^(8))
#
#   # Initialize the negative log likelihood computation
#   nloglik = ifelse(mod$nreg==0,  - log(mod$mypdf(mod$DependentVar[1],Parms$MargParms)),
#                    - log(mod$mypdf(mod$DependentVar[1], Parms$ConstMargParm, Parms$DynamMargParm[1])))
#
#   # Compute covariance up to lag n-1
#   gt    = as.vector(ARMAacf(ar = Parms$AR, ma = Parms$MA, lag.max = mod$n))
#
#   # Compute coefficients of Innovations Algorithm see 5.2.16 and 5.3.9 in in Brockwell Davis book
#   IA    = innovations.algorithm(gt)
#   Theta = IA$thetas
#   Rt    = sqrt(IA$v)
#
#   # allocate matrices for weights, particles and innovations which are equal to Z-Zhat
#   w     = matrix(0, mod$n, mod$ParticleNumber)
#   Z     = matrix(0, max(mod$ARMAModel), mod$ParticleNumber)
#   Inn   = matrix(0, max(mod$ARMAModel), mod$ParticleNumber)
#
#   # particle filter weights
#   w[1,]   = rep(1,mod$ParticleNumber)
#
#   # Compute the first integral limits Limit$ a and Limit$b
#   Limit = ComputeLimits(mod, Parms, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#
#   # Generate N(0,1) variables restricted to (ai,bi),i=1,...n
#   #Z[1,]   = SampleTruncNormParticles(mod, Limit$a, Limit$b, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#   Z[1,]   = SampleTruncNormParticles(mod, Limit, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#
#   # Compute the first innovation (Zhat_1=0)
#   Inn[1,] = Z[1,]
#
#
#   for (t in 2:mod$n){
#
#     # Compute the latent Gaussian predictions Zhat_t using Innovations Algorithm - see 5.3.9 in Brockwell Davis book
#     if(mod$ParticleNumber==1){
#       if(t==2){
#         Zhat  =         Inn[1:(min(t-1,mod$nMA)),] %*% Theta[[t-1]][1:(min(t-1,mod$nMA))]
#       }else{
#         Zhat  = colSums(Inn[1:(min(t-1,mod$nMA)),] %*% Theta[[t-1]][1:(min(t-1,mod$nMA))])
#       }
#     }else{
#       if(t==2){
#         Zhat  =         Inn[1:(min(t-1,mod$nMA)),] * Theta[[t-1]][1:(min(t-1,mod$nMA))]
#       }else{
#         Zhat  = colSums(Inn[1:(min(t-1,mod$nMA)),] * Theta[[t-1]][1:(min(t-1,mod$nMA))])
#       }
#     }
#
#     # Compute integral limits
#     Limit = ComputeLimits(mod, Parms, t, Zhat, Rt)
#
#     # Sample truncated normal particles
#     #Znew  = SampleTruncNormParticles(mod, Limit$a, Limit$b, t, Zhat, Rt)
#     Znew  = SampleTruncNormParticles(mod, Limit, t, Zhat, Rt)
#
#     # update weights
#     #w[t,] = ComputeWeights(mod, Limit$a, Limit$b, t, w[(t-1),])
#     w[t,] = ComputeWeights(mod, Limit, t, w[(t-1),])
#
#
#     # check me: break if I got NA weight
#     if (any(is.na(w[t,]))| sum(w[t,])==0 ){
#       message(sprintf('WARNING: Some of the weights are either too small or sum to 0'))
#       return(10^8)
#     }
#
#     # Resample the particles using common random numbers
#     old_state1 = get_rand_state()
#     Znew = ResampleParticles(mod, w, t, Znew)
#     set_rand_state(old_state1)
#
#     # Compute the new Innovation
#     InnNew = Znew - Zhat
#
#     # Combine current particles, with particles from previous iterations
#     Inn[1:min(t,mod$nMA),] = rbind(matrix(InnNew,ncol=mod$ParticleNumber), matrix(Inn[1:min(t-1,mod$nMA-1),],ncol = mod$ParticleNumber))
#
#     # update likelihood
#     nloglik = nloglik - log(mean(w[t,]))
#
#   }
#
#   # for log-likelihood we use a bias correction--see par2.3 in Durbin Koopman, 1997
#   # nloglik = nloglik- (1/(2*N))*(var(na.omit(wgh[T1,]))/mean(na.omit(wgh[T1,])))/mean(na.omit(wgh[T1,]))
#
#   # if (nloglik==Inf | is.na(nloglik)){
#   #   nloglik = 10^8
#   # }
#
#
#   return(nloglik)
# }
#
# # older PF likelihood with resampling for ARMA(p,q)
# ParticleFilter_Res_ARMA_old = function(theta, mod){
#   #------------------------------------------------------------------------------------#
#   # PURPOSE:  Use particle filtering with resampling to approximate the likelihood
#   #           of the a specified count time series model with an underlying MA(1)
#   #           dependence structure.
#   #
#   # NOTES:    1. See "Latent Gaussian Count Time Series Modeling" for  more
#   #           details. A first version of the paper can be found at:
#   #           https://arxiv.org/abs/1811.00203
#   #           2. This function is very similar to LikSISGenDist_ARp but here
#   #           I have a resampling step.
#   #           3. The innovations algorithm here is the standard one, likely it will
#   #           be slower.
#   # INPUTS:
#   #    theta:            parameter vector
#   #    data:             data
#   #    ParticleNumber:   number of particles to be used.
#   #    Regressor:        independent variable
#   #    CountDist:        count marginal distribution
#   #    epsilon           resampling when ESS<epsilon*N
#   #
#   # OUTPUT:
#   #    loglik:           approximate log-likelihood
#   #
#   #
#   # AUTHORS: James Livsey, Vladas Pipiras, Stefanos Kechagias,
#   # DATE:    July 2020
#   #------------------------------------------------------------------------------------#
#
#   old_state <- get_rand_state()
#   on.exit(set_rand_state(old_state))
#
#   # Retrieve parameters and save them in a list called Parms
#   Parms = RetrieveParameters(theta,mod)
#   #print(Parms$AR)
#   # check for causality and invertibility
#   if( CheckStability(Parms$AR,Parms$MA) ){
#     mod$ErrorMsg = sprintf('WARNING: The ARMA polynomial must be causal and invertible.')
#     warning(mod$ErrorMsg)
#     return(mod$loglik_BadValue1)
#   }
#
#   # Initialize the negative log likelihood computation
#   nloglik = ifelse(mod$nreg==0,  - log(mod$mypdf(mod$DependentVar[1],Parms$MargParms)),
#                    - log(mod$mypdf(mod$DependentVar[1], Parms$ConstMargParm, Parms$DynamMargParm[1,])))
#
#   # retrieve AR, MA orders and their max
#   m = max(mod$ARMAModel)
#   p = mod$ARMAModel[1]
#   q = mod$ARMAModel[2]
#
#   # Compute ARMA covariance up to lag n-1
#   a        = list()
#   if(!is.null(Parms$AR)){
#     a$phi = Parms$AR
#   }else{
#     a$phi = 0
#   }
#   if(!is.null(Parms$MA)){
#     a$theta = Parms$MA
#   }else{
#     a$theta = 0
#   }
#   a$sigma2 = 1
#   gamma    = itsmr::aacvf(a,mod$n)
#
#   # Compute coefficients of Innovations Algorithm see 5.2.16 and 5.3.9 in in Brockwell Davis book
#   #IA       = InnovAlg(Parms, gamma, mod)
#   IA       = innovations.algorithm(gamma)
#   Rt       = sqrt(IA$v)
#
#   # allocate matrices for weights, particles and predictions of the latent series
#   w        = matrix(0, mod$n, mod$ParticleNumber)
#   Z        = matrix(0, mod$n, mod$ParticleNumber)
#   Zhat     = matrix(0, mod$n, mod$ParticleNumber)
#
#   # initialize particle filter weights
#   w[1,]    = rep(1,mod$ParticleNumber)
#
#   # Compute the first integral limits Limit$a and Limit$b
#   Limit    = ComputeLimits(mod, Parms, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#
#   # Initialize the particles using N(0,1) variables truncated to the limits computed above
#   Z[1,]    = SampleTruncNormParticles(mod, Limit, 1, rep(0,1,mod$ParticleNumber), rep(1,1,mod$ParticleNumber))
#
#
#   for (t in 2:mod$n){
#
#     # compute Zhat_t
#     Zhat[t,] = ComputeZhat_t(mod, IA, Z, Zhat,t, Parms)
#
#     # Compute integral limits
#     Limit = ComputeLimits(mod, Parms, t, Zhat[t,], Rt)
#
#     # Sample truncated normal particles
#     Znew  = SampleTruncNormParticles(mod, Limit, t, Zhat[t,], Rt)
#
#     # update weights
#     w[t,] = ComputeWeights(mod, Limit, t, w[(t-1),])
#
#     # check me: break if I got NA weight
#     if (any(is.na(w[t,]))| sum(w[t,])==0 ){
#       #print(t)
#       #print(w[t,])
#       message(sprintf('WARNING: Some of the weights are either too small or sum to 0'))
#       return(mod$loglik_BadValue2)
#     }
#
#     # Resample the particles using common random numbers
#     old_state1 = get_rand_state()
#     Znew = ResampleParticles(mod, w, t, Znew)
#     set_rand_state(old_state1)
#
#     # save the current particle
#     Z[t,]   = Znew
#
#     # update likelihood
#     nloglik = nloglik - log(mean(w[t,]))
#
#   }
#
#   # for log-likelihood we use a bias correction--see par2.3 in Durbin Koopman, 1997
#   # nloglik = nloglik- (1/(2*N))*(var(na.omit(wgh[T1,]))/mean(na.omit(wgh[T1,])))/mean(na.omit(wgh[T1,]))
#
#   # if (nloglik==Inf | is.na(nloglik)){
#   #   nloglik = 10^8
#   # }
#
#
#   return(nloglik)
# }


#' Dimension function for vector/matrix inputs
#' Treats vectors as length x 1 column vectors
#'
#' @param x vector (column matrix) or matrix
#'
#' @return the vector's dimension
#' @export
#'
DIM <- function(x){
  if (is.null(dim(x)))
    c(length(x), 1)
else
  dim(x)
}
